---
layout: post
categories: posts
title: FAIR Principles    
featured-image: /images/DID.jpeg
tags: [FAIR, Findability, Accessibility, Interoperability, and Reusability]
date-string: MARCH 2021
---

# FAIR Principles

A March 2016 publication by a consortium of scientists and organizations specified the "FAIR Guiding Principles for scientific data management and stewardship" in Scientific Data, using FAIR as an acronym and making the concept easier to discuss.

The authors intended to provide guidelines to improve the findability, accessibility, interoperability, and reuse of digital assets. The FAIR principles emphasize machine-actionability (i.e., the capacity of computational systems to find, access, interoperate, and reuse data with none or minimal human intervention) because humans increasingly rely on computational support to deal with data as a result of the increase in volume, complexity, and creation speed of data.

Factors affecting value of data 
There are national initiatives underway to support the ubiquitous exchange, sharing, and reuse of operational clinical data stored in EHRs Just because information can be exchanged electronically does not mean it can be digitally analyzed. Data need to be structured using clear electronic standards such as FAIR principles to improve the management and organization of health care data for professionals and clinicians to analyze and use data for critical decision making.


Nature of data 
Data access and availability 
Data access and availability refer to how quickly data is available to the buyer after it is created. Contemporaneous data is one that is immediately available for analysis. 
The age of the data can increase or decrease its value, but in general, rapid access to fresh data is thought to be of more value, as it provides real-time insights and enables timely informed decisions. 
Exclusivity or scarcity 
When data is only available from a single source, the party holding the data acquires monopoly pricing power over the data. The larger the business disruption caused by losing access to such data, the more significant will be the seller’s ability to charge a premium price. 
Access to data can be limited via physical barriers, expense of the collection process, business strategy or contractual restrictions. 
Granularity of data 
In general, the increased granularity or specificity of a data set increases its value by allowing the user to extract additional insights absent from aggregated data sets. 
In health care context, granularity translates into patient-and transaction-level data (e.g., medical product dispense, patient encounters, change in status, etc.). 
By analysing dispense data at a transaction level, pharmaceutical companies can hone in on dispensing patterns in specific zip codes — which would not be possible using aggregated data. 
Another example of value evident in granularity is genome sequencing data. By using predictive analytics on patient-level genome sequencing data, one can identify individual patients that are at high risk for certain conditions. Providers can then take preventative actions with the patient to decrease the likelihood of developing or worsening a medical condition. According to the 2016 McKinsey report: The Age of Analytics — Competing in a Data-Driven World, the impact from personalised medicine resulting from such data analytics could result in US$2tn to US$10tn in health care savings on a global basis. 
Source or seller 
Data that is purchased directly from the party that generated it will generally be more valuable as compared with the data purchased from a reseller. This is due to data accuracy. 
Altered or inaccurate data may result in misleading conclusions and faulty decisions. It is presumed that data purchased from a reseller is at a higher risk of having been tampered with or altered, when compared with data purchased from the original source. 
It should be noted, however, that many third-party data aggregators and resellers may clean and organise data to make it easier to analyse. The incremental increase of data value resulting from this analytical process is addressed in 5.1.2 of the framework and should be weighted against the increased risk of data alteration


Data quality, maturity and embedded analytics insights 
Data maturity 
The concept of data maturity addresses the progression of a data set along the data analytics process. 
More often than not, raw data sets are disorganised and complex, or lack structure. To extract insights from it, one must clean, organise and analyse it. 
Complexity of data capture 
Data capture 
The value of data is also affected by the effort and risk associated with capturing it. In general, data that is auto captured as a by-product of everyday business processes would generally be perceived as less valuable — on a cost-approach basis — than the data that is collected in a separate process with human intervention and by including or not including other incremental resources involved in the process. 
Accessibility of data 
Generally speaking, any restrictions on access to data that is perceived by multiple parties as useful to their decision-making processes will increase its value. Hence, the value of data that is freely available to any interested party would be lower than the value of the data that calls for certain qualifications from the potential buyer, ranging from a monetary payment or through qualifications, such as citizenship or a need to submit a use case request. 
In the case of government or public data sets, a monetary payment set for accessing may not reflect full market value of the data across multiple user groups. When using such payments as price benchmarks in an fair market value (FMV) assessment, careful consideration should be given to understanding the price formation of such access restrictions and their consistency with true market value of the data. 
For example, government data, such as census results, allows for ready resource allocation, capital investment planning, policy-making and monitoring numerous other benefits. In the G20 countries, open-sourced data is estimated to be valued between US$700bn and US$950bn per annum according to a 2014 study by Lateral Economics. 
Use or application 
Use and potential impact 
The ultimate determinant of data value is its use and application — its power to inform decisions. These may range from ongoing daily operating decisions of a pharmaceutical company, to reducing time to market for new medicines and improving patient adherence to therapy. 
The more uses the data set has to multiple buyers, and the more risky and impactful are the resulting decisions on the user’s long-term strategy and operations, the more valuable it is to the buyers. 

https://www.go-fair.org/fair-principles/

https://portagenetwork.ca/
