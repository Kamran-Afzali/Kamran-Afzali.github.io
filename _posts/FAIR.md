---
layout: post
categories: posts
title: FAIR Principles    
featured-image: /images/FAIR.png
tags: [FAIR, Findability, Accessibility, Interoperability, Reusability]
date-string: MARCH 2021
---

# FAIR Principles

## Factors Affecting Value of Healthcare Data 

Several initiatives are underway to support the seemless sharing, exchange, and reuse of healthcare data whether stored in electronic health records or measurment devices. The goal of these initiatives is  to improve the healthcare data in a way that it can be exchanged electronically and analyzed digitally. To attain this goal and to improve the management and organization of health care data, it needs to be structured based on operational electronic standards  such as FAIR principles that insures clinicians and professionals use suitable data for critical decision making.  

## FAIR Principles

A publication by a consortium of scientists and organizations back in March 2016 specified the details of "FAIR Guiding Principles for scientific data management and stewardship" in Scientific Data, using FAIR as an acronym and making the concept easier to discuss. The authors of this document intended to promote general guidelines that improve the Findability, Accessibility, Interoperability, and Reuse of digital assets such as electronic health records. The FAIR principles emphasize that computerized systems should Find, Access, Interoperate, and Reuse the data with none or minimal human intervention (i.e., the capacity of machine-actionability) because humans increasingly rely on digital support to handel big data as a result of the increase in velocity, volume, variety, and complexity of data. The FAIR principles apply to data and supporting infrastructure but also to metadata, with most of the requirements for Accessibility and Findability is achievable through the metadata. In contrast, Interoperability and Reuse require more efforts at the data level. 

The FAIR Data Principles make data more valuable as it is easier to find through unique identifiers and easier to combine and integrate thanks to the formal shared knowledge representation. Such data is easier to reuse, repurpose and share because machines have the means to understand where data comes from and what it is about. It also accelerates research, boosts cooperation and facilitates reuse in scientific research. Policymakers and stakeholders have seen its value in driving innovation and many have embraced these principles.

Focusing on the research based on open science, the FAIR principles received the support of  G20 leaders during their 2016 summit. In the United States, the Office of Science at the Department of Energy announced in April 2020 a total of US$8.5 million for new research aimed at advancing the FAIR Data Principles in Artificial Intelligence (AI) research and development. The European Union has also embraced them and had an expert group report on how to turn FAIR into reality.


## Findable

The first step in (re)using data is to find them. Metadata and data should be easy to find for both humans and computers. Machine-readable metadata are essential for automatic discovery of datasets and services.

+ F1. (Meta)data are assigned a globally unique and persistent identifier

+ F2. Data are described with rich metadata (defined by R1 below)

+ F3. Metadata clearly and explicitly include the identifier of the data they describe

+ F4. (Meta)data are registered or indexed in a searchable resource

## Accessible

Once the user finds the required data, she/he needs to know how can they be accessed, possibly including authentication and authorisation.

+ A1. (Meta)data are retrievable by their identifier using a standardised communications protocol

  + A1.1 The protocol is open, free, and universally implementable

  + A1.2 The protocol allows for an authentication and authorisation procedure, where necessary

+ A2. Metadata are accessible, even when the data are no longer available

## Interoperable

The data usually need to be integrated with other data. In addition, the data need to interoperate with applications or workflows for analysis, storage, and processing.

+ I1. (Meta)data use a formal, accessible, shared, and broadly applicable language for knowledge representation.

+ I2. (Meta)data use vocabularies that follow FAIR principles

+ I3. (Meta)data include qualified references to other (meta)data

## Reusable

The ultimate goal of FAIR is to optimise the reuse of data. To achieve this, metadata and data should be well-described so that they can be replicated and/or combined in different settings.

+ R1. (Meta)data are richly described with a plurality of accurate and relevant attributes

  + R1.1. (Meta)data are released with a clear and accessible data usage license

  + R1.2. (Meta)data are associated with detailed provenance

  + R1.3. (Meta)data meet domain-relevant community standards




## Nature of data 
## Data access and availability 
Data access and availability refer to how quickly data is available to the buyer after it is created. Contemporaneous data is one that is immediately available for analysis. 
The age of the data can increase or decrease its value, but in general, rapid access to fresh data is thought to be of more value, as it provides real-time insights and enables timely informed decisions. 
Exclusivity or scarcity 
When data is only available from a single source, the party holding the data acquires monopoly pricing power over the data. The larger the business disruption caused by losing access to such data, the more significant will be the seller’s ability to charge a premium price. 
Access to data can be limited via physical barriers, expense of the collection process, business strategy or contractual restrictions. 
Granularity of data 
In general, the increased granularity or specificity of a data set increases its value by allowing the user to extract additional insights absent from aggregated data sets. 
In health care context, granularity translates into patient-and transaction-level data (e.g., medical product dispense, patient encounters, change in status, etc.). 
By analysing dispense data at a transaction level, pharmaceutical companies can hone in on dispensing patterns in specific zip codes — which would not be possible using aggregated data. 
Another example of value evident in granularity is genome sequencing data. By using predictive analytics on patient-level genome sequencing data, one can identify individual patients that are at high risk for certain conditions. Providers can then take preventative actions with the patient to decrease the likelihood of developing or worsening a medical condition. According to the 2016 McKinsey report: The Age of Analytics — Competing in a Data-Driven World, the impact from personalised medicine resulting from such data analytics could result in US$2tn to US$10tn in health care savings on a global basis. 
Source or seller 
Data that is purchased directly from the party that generated it will generally be more valuable as compared with the data purchased from a reseller. This is due to data accuracy. 
Altered or inaccurate data may result in misleading conclusions and faulty decisions. It is presumed that data purchased from a reseller is at a higher risk of having been tampered with or altered, when compared with data purchased from the original source. 
It should be noted, however, that many third-party data aggregators and resellers may clean and organise data to make it easier to analyse. The incremental increase of data value resulting from this analytical process is addressed in 5.1.2 of the framework and should be weighted against the increased risk of data alteration


## Data quality, maturity and embedded analytics insights 
## Data maturity 
The concept of data maturity addresses the progression of a data set along the data analytics process. 
More often than not, raw data sets are disorganised and complex, or lack structure. To extract insights from it, one must clean, organise and analyse it. 
Complexity of data capture 
## Data capture 
The value of data is also affected by the effort and risk associated with capturing it. In general, data that is auto captured as a by-product of everyday business processes would generally be perceived as less valuable — on a cost-approach basis — than the data that is collected in a separate process with human intervention and by including or not including other incremental resources involved in the process. 
## Accessibility of data 
Generally speaking, any restrictions on access to data that is perceived by multiple parties as useful to their decision-making processes will increase its value. Hence, the value of data that is freely available to any interested party would be lower than the value of the data that calls for certain qualifications from the potential buyer, ranging from a monetary payment or through qualifications, such as citizenship or a need to submit a use case request. 
In the case of government or public data sets, a monetary payment set for accessing may not reflect full market value of the data across multiple user groups. When using such payments as price benchmarks in an fair market value (FMV) assessment, careful consideration should be given to understanding the price formation of such access restrictions and their consistency with true market value of the data. 
For example, government data, such as census results, allows for ready resource allocation, capital investment planning, policy-making and monitoring numerous other benefits. In the G20 countries, open-sourced data is estimated to be valued between US$700bn and US$950bn per annum according to a 2014 study by Lateral Economics. 
## Use or application 
## Use and potential impact 
The ultimate determinant of data value is its use and application — its power to inform decisions. These may range from ongoing daily operating decisions of a pharmaceutical company, to reducing time to market for new medicines and improving patient adherence to therapy. 
The more uses the data set has to multiple buyers, and the more risky and impactful are the resulting decisions on the user’s long-term strategy and operations, the more valuable it is to the buyers. 

https://www.go-fair.org/fair-principles/

https://portagenetwork.ca/

Wise, J., de Barron, A. G., Splendiani, A., Balali-Mood, B., Vasant, D., Little, E., ... & Hedley, V. (2019). Implementation and relevance of FAIR data principles in biopharmaceutical R&D. Drug discovery today, 24(4), 933-938.  https://www.sciencedirect.com/science/article/pii/S1359644618303039

Jansen, P., van den Berg, L., van Overveld, P., & Boiten, J. W. (2019). Research data stewardship for healthcare professionals. Fundamentals of Clinical Data Science, 37-53.https://www.ncbi.nlm.nih.gov/books/NBK543528/
