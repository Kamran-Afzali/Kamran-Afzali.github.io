## Neuro-Imaging Data

Brain occupies space, so it is possible to collect data on how it fills space, also called volume data. All the volume data needed to create the complete, 3D image of the brain, recorded at one single timepoint is called a *volume*. The data is measured in voxels, which are like the pixels used to display images on your screen, only in 3D. Each voxel has a specific dimension, with it being the same dimension from all sides (isotropic). Each voxel contains one value which stands for the average signal measured at the given location. A standard anatomical volume, with an isotropic voxel resolution of 1mm contains almost 17 million voxels, which are arranged in a 3D matrix of 256 x 256 x 256 voxels. As the scanner can’t measure the whole volume at once it has to measure portions of the brain sequentially in time. This is done by measuring one plane of the brain (generally the horizontal one) after the other. The resolution of the measured volume data, therefore, depends on the in-plane resolution (the size of the squares in the above image), the number of slices and their thickness (how many layers), and any possible gaps between the layers. The quality of the measured data depends on the resolution and the following parameters:

+ Repetition time (TR): time required to scan one volume

+ Acquisition time (TA): time required to scan one slice. TA = TR - (TR/number of slices)

+ Field of View (FOV): defines the extent of a slice, e.g. 256mm x 256mm

### Specifics of MRI Data

MRI scanners output their neuroimaging data in a raw data format with which most analysis packages cannot work. DICOM is a common, standardized, raw medical image format. Raw data is saved in k-space format, and it needs to be converted into a format that the analysis packages can use. The most frequent format for newly generated data is called NIfTI. MRI data formats will have an image and a header part. For NifTI format, they are in the same file (.nii-file), whereas in the older Analyze format, they are in separate files (.img and .hdr-file). The image is the actual data and is represented by a 3D matrix that contains a value (e.g. gray value) for each voxel. The header contains information about the data like voxel dimension, voxel extend in each dimension, number of measured time points, a transformation matrix that places the 3D matrix from the image part in a 3D coordinate system, etc.

### Modalities of MRI Data

There are many different kinds of acquisition techniques. But the most common ones are structural magnetic resonance imaging (sMRI), functional magnetic resonance imaging (fMRI) and diffusion tensor imaging (DTI).

#### sMRI (structural MRI)

Structural magnetic resonance imaging (sMRI) is a technique for measuring the anatomy of the brain. By measuring the amount of water at a given location, sMRI is capable of acquiring a detailed anatomical picture of our brain. This allows us to accurately distinguish between different types of tissue, such as gray and white matter, with grey matter structures are seen in dark, and the white matter structures in bright colors. Structural images are high-resolution images of the brain that are used as reference images for multiple purposes, such as corregistration, normalization, segmentation, and surface reconstruction. As the anatomy is not supposed to change while the person is in the scanner, a higher resolution can be used for recording anatomical images, with a voxel extent of 0.2 to 1.5mm, depending on the strength of the magnetic field in the scanner, e.g. 1.5T, 3T or 7T. 

#### fMRI (functional MRI)

Functional magnetic resonance imaging (fMRI) is a technique for measuring brain activity by detecting the changes in blood oxygenation and blood flow that occur in response to neural activity. Brain needs a lot of energy to sustain its functionality, and increased activity at a location increases the local energy consumption in the form of oxygen which is carried by the blood. Therefore, increased function results in increased blood flow towards the energy consuming location. Immediately after neural activity the blood oxygen level decreases, known as the initial dip, because of the local energy consumption. This is followed by increased flow of new and oxygen-rich blood towards the energy consuming region. After 4-6 seconds a peak of blood oxygen level is reached. After no further neuronal activation takes place the signal decreases again and typically undershoots, before rising again to the baseline level. The blood oxygen level is exactly what we measure with fMRI. The MRI Scanner is able to measure the changes in the magnetic field caused by the difference in the magnetic susceptibility of oxygenated (diamagnetic) and deoxygenated (paramagnetic) blood. The signal is therefore called the Blood Oxygen Level Dependent (BOLD) response. Because the BOLD signal has to be measured quickly, the resolution of functional images is normally lower (2-4mm) than the resolution of structural images (0.5-1.5mm). But this depends strongly on the strength of the magnetic field in the scanner, e.g. 1.5T, 3T or 7T. In a functional image, the gray matter is seen as bright and the white matter as dark colors, which is the exact opposite to structural images.

##### **fMRI designs**

+ Event-related design
  + Event-related means that stimuli are administered to the subjects in the scanner for a short period. The stimuli are only administered briefly and generally in random order. Stimuli are typically visual, but audible or other sensible stimuli could also be used. This means that the BOLD response consists of short bursts of activity, which should manifest as peaks.

+ Block design
  + If multiple stimuli of a similar nature are shown in a block, or phase, of 10-30 seconds, that is a block design. Such a design has the advantages that the peak in the BOLD signal is not just attained for a short period but elevated for a longer time, creating a plateau in the graph. This makes it easier to detect an underlying activation increase.

+ Resting-state design
  + Resting-state designs acquire data in the absence of stimulation. Subjects are asked to lay still and rest in the scanner without falling asleep. The goal of such a scan is to record brain activation in the absence of an external task. This is sometimes done to analyze the functional connectivity of the brain.

#### dMRI (diffusion MRI)

Diffusion imaging is done to obtain information about the brain’s white matter connections. There are multiple modalities to record diffusion images, such as diffusion tensor imaging (DTI), diffusion spectrum imaging (DSI), diffusion weighted imaging (DWI) and diffusion functional MRI (DfMRI). By recording the diffusion trajectory of the molecules (usually water) in a given voxel, one can make inferences about the underlying structure in the voxel. For example, if one voxel contains mostly horizontal fiber tracts, the water molecules in this region will mostly move in a horizontal manner, as they can’t move vertically because of this neural barrier. There are many different diffusion measurements, such as mean diffusivity (MD), fractional anisotropy (FA) and Tractography. Each measurement gives different insights into the brain’s neural fiber tracts. Diffusion MRI is a rather new field in MRI and still has some problems with its sensitivity to correctly detect fiber tracts and their underlying orientation. For more about diffusion MRI see the Diffusion MRI Wikipedia page.

### Image processing

Neuroimaging is all about images and the fundamental type of data that most neuroimaging researchers use are images of brains. Here we’ll give a broad introduction to image processing to develop an intuition for the kind of things that you might think about when you are doing image processing more specifically: image segmentation and image registration. These are particularly common in the analysis of neuroimaging data and a detailed understanding of these operations should serve you well, even if you ultimately use specialized off-the-shelf software tools for these operations, rather than implementing them yourself.

### Images are arrays

Neuroimaging objects can be represented as Numpy arrays. However, one thing that distinguishes images from other kinds of data is that spatial relationships are crucially important for the analysis of images. That is because neighboring parts of the image usually represent objects that are near each other in the physical world as well. Image-processing algorithms should use these neighborhood relationships. Conversely, in some cases, these neighborhood relationships induce constraints on the operations one can do with the images because we’d like to keep these neighborhood relationships constant even if we put the image through various transformations. 

### Images can have two dimensions or more

We typically think of images as having two dimensions, because when we look at images, we are usually viewing them on a two-dimensional surface: on the screen of a computer or a page. But some kinds of images have more than two dimensions. For example, in most cases where we are analyzing brain images, we are interested in the full three-dimensional image of the brain. This means that brain images are usually (at least) three-dimensional. Again, under our definition of an image, this is fine. So long as the spatial relationships are meaningful and important in the analysis, it is fine that they extend over more than two dimensions.


### Image segmentation 

Here we are going to look at image segmentation. Image segmentation divides the pixels or voxels of an image into different groups. For example, you might divide a neuroimaging dataset into parts that contain the brain itself and parts that are non-brain (e.g., background, skull, etc.). Generally speaking, segmenting an image allows us to know where different objects are in an image and allows us to separately analyze parts of the image that contain particular objects of interest. For example, we don’t want to analyze the voxels that contain parts of the subject’s skull or in the ventricles. To be able to select only the voxels that are in the brain proper, we need to segment the image into brain and non-brain. There are a few different approaches to this problem.

#### Intensity-based segmentation

The first and simplest approach to segmentation is to use the distribution of pixel intensities as a basis for segmentation For instance, the parts of the image that contain the brain are brighter – have higher intensity values. The parts of the image that contain the background are dark and contain low-intensity values. One way of looking at the intensity values in the image is using a histogram. For this it is enough to flat the representation of the image array, which unfolds the two-dimensional array into a one-dimensional array.

#### Otsu’s method

A classic approach to this problem is now known as “Otsu’s method”, after the Japanese engineer Nobuyuki Otsu, who invented it in the 1970s. The method relies on a straightforward principle: find a threshold that minimizes the variance in pixel intensities within each class of pixels (for example, brain and non-brain). This principle is based on the idea that pixels within each of these segments should be as similar as possible to each other, and as different as possible from the other segment. It also turns out that this is a very effective strategy for many other cases where you would like to separate the background from the foreground. This method is looking for a threshold that would minimize the total variance in pixel intensities within each class, or “intraclass variance”. This has two components: The first is the variance of pixel intensities in the background pixels, weighted by the proportion of the image that is in the background. The other is the variance of pixel intensities in the foreground, weighted by the proportion of pixels belonging to the foreground. To find this threshold value, Otsu’s method relies on the following procedure: Calculate the intraclass variance for every possible value of the threshold and find the candidate threshold that corresponds to the minimal intraclass variance. The foreground contribution to the intraclass variance is the variance of the intensities among the foreground pixels. The background contribution to the intraclass variance is the variance of the intensities in the background pixels, multiplied by the number of background pixels (with very similar code for each of these). The intraclass variance is the sum of these two. After running through several candidates, the value stored in the threshold variable will be the value of the candidate that corresponds to the smallest possible intraclass variance.

#### Edge-based segmentation

Threshold-based segmentation assumes that different parts of the image that should belong to different segments should have different distributions of pixel values. But this approach does not take advantage of the spatial layout of the image. Another approach to segmentation uses the fact that parts of the image that should belong to different segments are usually separated by edges. What is an edge? Usually, these are contiguous boundaries between parts of the image that look different from each other. These differences can be differences in intensity, but also differences in texture, pattern, and so on. In a sense, finding edges and segmenting the image are a bit of a chicken and egg: if you knew where different parts of the image were, you wouldn’t have to segment them. Nevertheless, in practice, using algorithms that specifically focus on finding edges in the image can be useful to perform the segmentation. Finding edges in images has been an important part of computer vision for about as long as there has been a field called computer vision. Sophisticated edge detection algorithms can use this, together with a few more steps, to more robustly find edges that extend in space. One such algorithm is the Canny edge detector, named after its inventor, John Canny, a computer scientist, and computer vision researcher. The algorithm takes several steps that include smoothing the image to get rid of noisy or small parts, finding the local gradients of the image (for example, with the Sobel filter) and then thresholding this image, selecting edges that are connected to other strong edges and suppressing edges that are connected to weaker edges. 

### Registration

Another major topic is image registration of two images that contain parts that should overlap with each other, but do not. The key is to identify a way to quantify how aligned the two images are to each other. Our visual systems are very good at identifying when two images are aligned, however, we need to create an alignment measure. These measures are often called cost functions. There are many different types of cost functions depending on the types of images that are being aligned. For example, a common cost function is called minimizing the sum of the squared differences and is similar to how regression lines are fit to minimize deviations from the observed data. This measure works best if the images are of the same type and have roughly equivalent signal intensities. Let’s create another interactive plot and find the optimal X & Y translation parameters that minimize the difference between a two-dimensional target image to a reference image.

#### Affine registration

The first kind of registration that we will talk about corrects for changes between the images that affect all of the content in the image.  In the case of brain imaging, a global change to the position of the brain is mostly related to the fact that the subject moved between the time in which each of the images was acquired. An affine registration can correct for global differences between images by applying a combination of transformations on the image: translating the entire image up or down, to the left or the right; rotating it clockwise or counter-clockwise, scaling the image (zooming in or out), and shearing the image. While most of these are straightforward to understand, the last one is a bit more complex: image shearing means that different parts of the image move to different degrees. In one example of a horizontal shear, the top row of pixels doesn’t move at all, the second row moves by some amount, the third row moves by that amount, and a small amount more, and so on. A larger amount of shear means that as we go down the image subsequent rows of pixels move by more and more.

#### Diffeomorphic registration

Global registration approach does well in registering the overall structure of one image to another, but it doesn’t necessarily capture differences in small details. Another family of registration algorithms registers different parts of the image separately. In principle, you can imagine that each pixel in the first image could independently move to any location in the second image. But using this unconstrained approach, in which you can move every pixel in one image to any location in the other image, you haven’t registered the images to each other, you’ve replaced them. Diffeomorphic registration is an approach that balances this flexibility with constraints. In principle, every pixel/voxel in the moving image could be moved to overlap with any pixel/voxel in the static image, but neighboring pixels/voxels are constrained to move by a similar amount. That is, the mapping between the moving and the static image varies smoothly in space. 


### References
+ [Dartbrains](https://dartbrains.org)
+ [Neuroimaging-data-science](http://neuroimaging-data-science.org)
