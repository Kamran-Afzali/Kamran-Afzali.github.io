# On-Policy vs Off-Policy Reinforcement Learning: SARSA vs Q-Learning in R

## Introduction

In model-free reinforcement learning (RL), agents learn optimal policies directly from experience without a model of the environment's dynamics. Two key approaches are **on-policy** and **off-policy** methods, exemplified by **SARSA** (State-Action-Reward-State-Action) and **Q-Learning**, respectively. This post explores the differences between these methods, focusing on their theoretical foundations, practical implications, and implementation in R. We use the same 10-state, 2-action environment from the previous post to compare how SARSA and Q-Learning learn policies and adapt to environmental changes, such as outcome devaluation. Mathematical formulations and R code are provided to illustrate the concepts.

## Theoretical Background

Both SARSA and Q-Learning aim to estimate the action-value function \( Q^\pi(s,a) \), the expected discounted return for taking action \( a \) in state \( s \) and following policy \( \pi \):

\[
Q^\pi(s,a) = \mathbb{E}_\pi \left[ \sum_{t=0}^\infty \gamma^t R_{t+1} \mid S_0 = s, A_0 = a \right]
\]

where \( \gamma \in [0,1] \) is the discount factor, and \( R_{t+1} \) is the reward at time \( t+1 \).

### Q-Learning (Off-Policy)

Q-Learning is an **off-policy** method, meaning it learns the optimal policy \( \pi^* \) regardless of the policy used for exploration (e.g., \( \epsilon \)-greedy). The update rule is:

\[
Q(s,a) \leftarrow Q(s,a) + \alpha \left( r + \gamma \max_{a'} Q(s', a') - Q(s,a) \right)
\]

where \( \alpha \) is the learning rate, \( r \) is the reward, \( s' \) is the next state, and \( \max_{a'} Q(s', a') \) estimates the value of the next state assuming the optimal action. This bootstrapping makes Q-Learning converge to the optimal action-value function \( Q^*(s,a) \).

### SARSA (On-Policy)

SARSA is an **on-policy** method, meaning it learns the value of the policy being followed, including exploration. The update rule is:

\[
Q(s,a) \leftarrow Q(s,a) + \alpha \left( r + \gamma Q(s', a') - Q(s,a) \right)
\]

where \( a' \) is the action actually taken in state \( s' \) according to the current policy (e.g., \( \epsilon \)-greedy). SARSA updates \( Q \) based on the next state-action pair \( (s', a') \), making it sensitive to the exploration policy.

### Key Differences

| **Aspect**               | **SARSA (On-Policy)**                                                                 | **Q-Learning (Off-Policy)**                                                   |
|--------------------------|--------------------------------------------------------------------------------------|------------------------------------------------------------------------------|
| **Update Rule**          | Uses \( Q(s', a') \), where \( a' \) is sampled from the current policy.              | Uses \( \max_{a'} Q(s', a') \), assuming the optimal action in the next state. |
| **Policy Learning**      | Learns the value of the policy being followed (including exploration).               | Learns the optimal policy, independent of the exploration policy.             |
| **Exploration Impact**   | Exploration (e.g., \( \epsilon \)-greedy) affects learned Q-values.                   | Exploration does not affect learned Q-values (focuses on optimal actions).    |
| **Convergence**          | Converges to the optimal policy if exploration decreases (e.g., \( \epsilon \to 0 \)). | Converges to the optimal policy even with fixed exploration.                 |
| **Behavior**             | More conservative, accounts for exploration risks in the policy.                     | More aggressive, assumes optimal future actions.                             |

## Step 1: Environment Setup in R

We reuse the 10-state, 2-action environment with stochastic transitions and rewards from the previous post. The terminal state (state 10) yields higher rewards for action 1 (1.0) than action 2 (0.5).

```r
# Common settings
n_states <- 10
n_actions <- 2
gamma <- 0.9
terminal_state <- n_states

# Environment: transition + reward models
set.seed(42)
transition_model <- array(0, dim = c(n_states, n_actions, n_states))
reward_model <- array(0, dim = c(n_states, n_actions, n_states))

for (s in 1:(n_states - 1)) {
  transition_model[s, 1, s + 1] <- 0.9
  transition_model[s, 1, sample(1:n_states, 1)] <- 0.1
  transition_model[s, 2, sample(1:n_states, 1)] <- 0.8
  transition_model[s, 2, sample(1:n_states, 1)] <- 0.2
  for (s_prime in 1:n_states) {
    reward_model[s, 1, s_prime] <- ifelse(s_prime == n_states, 1.0, 0.1 * runif(1))
    reward_model[s, 2, s_prime] <- ifelse(s_prime == n_states, 0.5, 0.05 * runif(1))
  }
}

transition_model[n_states, , ] <- 0
reward_model[n_states, , ] <- 0

# Sampling function
sample_env <- function(s, a) {
  probs <- transition_model[s, a, ]
s_prime <- sample(1:n_states, 1, prob = probs)
  reward <- reward_model[s, a, s_prime]
  list(s_prime = s_prime, reward = reward)
}
```

## Step 2: Q-Learning Implementation in R

Q-Learning uses the off-policy update rule, selecting actions via an \( \epsilon \)-greedy policy:

```r
q_learning <- function(episodes = 1000, alpha = 0.1, epsilon = 0.1) {
  Q <- matrix(0, nrow = n_states, ncol = n_actions)
  
  for (ep in 1:episodes) {
    s <- 1
    while (TRUE) {
      a <- if (runif(1) < epsilon) sample(1:n_actions, 1) else which.max(Q[s, ])
      out <- sample_env(s, a)
      s_prime <- out$s_prime
      reward <- out$reward
      
      Q[s, a] <- Q[s, a] + alpha * (reward + gamma * max(Q[s_prime, ]) - Q[s, a])
      
      if (s_prime == n_states) break
      s <- s_prime
    }
  }
  list(Q = Q, policy = apply(Q, 1, which.max))
}
```

## Step 3: SARSA Implementation in R

SARSA follows the on-policy update rule, selecting the next action \( a' \) based on the current policy before updating \( Q \):

```r
sarsa <- function(episodes = 1000, alpha = 0.1, epsilon = 0.1) {
  Q <- matrix(0, nrow = n_states, ncol = n_actions)
  
  for (ep in 1:episodes) {
    s <- 1
    a <- if (runif(1) < epsilon) sample(1:n_actions, 1) else which.max(Q[s, ])
    while (TRUE) {
      out <- sample_env(s, a)
      s_prime <- out$s_prime
      reward <- out$reward
      
      # Select next action a' using the current policy
      a_prime <- if (runif(1) < epsilon) sample(1:n_actions, 1) else which.max(Q[s_prime, ])
      
      # SARSA update
      Q[s, a] <- Q[s, a] + alpha * (reward + gamma * Q[s_prime, a_prime] - Q[s, a])
      
      if (s_prime == n_states) break
      s <- s_prime
      a <- a_prime
    }
  }
  list(Q = Q, policy = apply(Q, 1, which.max))
}
```

## Step 4: Outcome Devaluation

We simulate a change in the environment by setting terminal state rewards to 0 and observe how policies behave without retraining:

```r
# Run algorithms before devaluation
ql_result_before <- q_learning()
sarsa_result_before <- sarsa()

# Devalue terminal reward
for (s in 1:(n_states - 1)) {
  reward_model[s, 1, n_states] <- 0
  reward_model[s, 2, n_states] <- 0
}

# Policies after devaluation (no retraining)
ql_policy_after <- ql_result_before$policy
sarsa_policy_after <- sarsa_result_before$policy
```

## Step 5: Visualizing Policies

We plot the policies before and after devaluation to compare SARSA and Q-Learning:

```r
plot_policy <- function(policy, label, col = "skyblue") {
  barplot(policy, names.arg = 1:n_states, col = col,
          ylim = c(0, 3), ylab = "Action (1=A1, 2=A2)",
          main = label)
  abline(h = 1.5, lty = 2, col = "gray")
}

par(mfrow = c(2, 2))
plot_policy(ql_result_before$policy, "Q-Learning Policy Before", "orange")
plot_policy(ql_policy_after, "Q-Learning Policy After", "lightcoral")
plot_policy(sarsa_result_before$policy, "SARSA Policy Before", "skyblue")
plot_policy(sarsa_policy_after, "SARSA Policy After", "lightgreen")
```

## Interpretation and Discussion

### Policy Differences
- **Q-Learning**: As an off-policy method, it learns the optimal policy, favoring actions that maximize future rewards (e.g., action 1, which yields a higher reward at the terminal state). Its policy is less sensitive to exploration noise, as it assumes optimal future actions.
- **SARSA**: As an on-policy method, it learns the value of the \( \epsilon \)-greedy policy, which includes exploratory actions. This makes SARSA more conservative, potentially avoiding risky actions that Q-Learning might favor.
- **Devaluation**: Both methods exhibit **habitual behavior** without retraining, retaining their original policies after the terminal reward is removed. This highlights a limitation of model-free methods compared to model-based approaches (e.g., dynamic programming), which adapt instantly.

### Practical Implications
- **SARSA**: Better suited for environments where the exploration policy must be accounted for, such as safety-critical systems (e.g., robotics), where risky exploratory actions could lead to poor outcomes.
- **Q-Learning**: Ideal for scenarios where the optimal policy is desired regardless of exploration, such as games or simulations where exploration does not incur real-world costs.
- **Convergence**: SARSA requires decaying exploration (e.g., \( \epsilon \to 0 \)) to converge to the optimal policy, while Q-Learning can converge even with fixed \( \epsilon \).

### Experimental Observations
- Before devaluation, Q-Learning may favor action 1 (higher terminal reward) more consistently due to its greedy updates. SARSA’s policy may show more variability due to its dependence on the exploration policy.
- After devaluation, both policies remain unchanged without retraining, illustrating their reliance on cached \( Q \)-values.

## Conclusion

SARSA and Q-Learning represent two paradigms in model-free RL: on-policy and off-policy learning. SARSA’s updates reflect the policy being followed, making it sensitive to exploration, while Q-Learning targets the optimal policy, ignoring exploration effects. The R implementations demonstrate these differences in a simple environment, and the devaluation experiment underscores their habitual nature. Future posts could explore advanced topics, such as SARSA(\(\lambda\)) or deep RL extensions.
