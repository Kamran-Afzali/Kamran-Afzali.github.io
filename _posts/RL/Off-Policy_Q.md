# Off-Policy Learning in Reinforcement Learning: Applications to Temporal Difference and Monte Carlo Methods

## Introduction

Reinforcement learning (RL) empowers agents to learn optimal decision-making strategies through interaction with an environment, guided by rewards. A key distinction in RL is between **on-policy** and **off-policy** learning. On-policy methods learn the value of the policy the agent follows, tightly coupling learning to the agent’s current behavior. In contrast, off-policy learning allows an agent to learn the value of an **optimal policy** while following a different, often exploratory, **behavior policy**. This decoupling offers remarkable flexibility, enabling agents to learn from diverse data sources—such as random exploration, human demonstrations, or past experiences—without being constrained by their current actions.

Imagine a robot navigating a maze to find a treasure. An on-policy method like SARSA learns only from the robot’s current path, refining its strategy based on the actions it takes. An off-policy method like Q-learning, however, can learn the fastest route to the treasure even if the robot wanders randomly, by evaluating what would have happened had it taken the best possible actions. This makes off-policy learning powerful for scenarios where exploration is costly or data comes from external sources, such as autonomous driving systems learning from human drivers’ logs or recommendation systems optimizing user engagement using historical data.

Off-policy learning is central to many RL successes, from Deep Q-Networks (DQNs) mastering Atari games to advanced actor-critic methods controlling robotic arms. This post explores off-policy learning applied to **Temporal Difference (TD)** and **Monte Carlo (MC)** methods, with mathematical foundations, R implementations, and a practical example. We demonstrate how off-policy methods learn optimal policies from exploratory behavior and examine their response to environmental changes, contrasting them with on-policy approaches.

## Theoretical Background

In RL, an agent interacts with an environment defined by states $S$, actions $A$, rewards $R$, and a discount factor $\gamma \in [0,1]$. The goal is to learn the action-value function $Q^\pi(s,a)$, the expected discounted return starting from state $s$, taking action $a$, and following policy $\pi$:

$$
Q^\pi(s,a) = \mathbb{E}_\pi \left[ \sum_{t=0}^\infty \gamma^t R_{t+1} \mid S_0 = s, A_0 = a \right]
$$

### Off-Policy Learning: Concept and Advantages

Off-policy learning distinguishes between the **behavior policy** $\mu(a|s)$, which generates actions for exploration (e.g., ε-greedy or random), and the **target policy** $\pi(a|s)$, which is the optimal policy being learned (e.g., greedy). This separation allows the agent to learn the value of the best possible actions even when its current actions are suboptimal or exploratory. For example, in a self-driving car scenario, an off-policy algorithm can learn an optimal driving policy from a dataset of human-driven trajectories, even if those trajectories include mistakes or cautious maneuvers. This flexibility makes off-policy learning ideal for applications where:

- **Data is diverse**: Learning from human demonstrations, logged data, or mixed policies (e.g., robotics, where a robot learns from a mix of expert and novice actions).
- **Exploration is costly**: In healthcare, where trial-and-error is risky, off-policy methods can use historical patient data to optimize treatment policies.
- **Reusability is key**: In recommendation systems, off-policy learning can improve suggestions by analyzing past user interactions, regardless of the policy used to collect them.

The trade-off is potential instability (e.g., in Q-learning with function approximation) or high variance (e.g., in off-policy MC due to importance sampling), but these methods enable robust learning in complex, real-world settings.

### Off-Policy Temporal Difference (Q-Learning)

Q-learning, a TD method, is inherently off-policy. It updates the Q-value using the maximum Q-value of the next state, assuming the optimal action is taken, regardless of the behavior policy:

$$
Q(s,a) \leftarrow Q(s,a) + \alpha \left( r + \gamma \max_{a'} Q(s', a') - Q(s,a) \right)
$$

where $\alpha$ is the learning rate, $r$ is the reward, and $s'$ is the next state. For instance, in a game like Pac-Man, Q-learning can learn the optimal move (e.g., chase a power pellet) even if the agent randomly explores other directions, by focusing on the best future outcomes.

### Off-Policy Monte Carlo

Monte Carlo methods are typically on-policy, averaging returns from episodes generated by the current policy. Off-policy MC uses **importance sampling** to learn the target policy’s value from episodes generated by a behavior policy. The return $G_t$ is weighted by the importance sampling ratio:

$$
\rho_t = \prod_{k=t}^T \frac{\pi(a_k|s_k)}{\mu(a_k|s_k)}
$$

where $\pi$ is the target (greedy) policy, $\mu$ is the behavior policy, and $T$ is the episode length. The Q-value update is:

$$
Q(s,a) \leftarrow Q(s,a) + \alpha \left( \rho_t G_t - Q(s,a) \right)
$$

For example, in a stock trading scenario, off-policy MC can learn an optimal trading strategy from a dataset of random trades by reweighting returns to reflect what a greedy strategy would have earned. This makes it suitable for offline RL settings but can suffer from high variance if the behavior and target policies diverge significantly.

## Step 1: Defining the Environment in R

We use a 10-state, 2-action environment with stochastic transitions and rewards, where the terminal state (state 10) initially yields a reward.

```r
# Common settings
n_states <- 10
n_actions <- 2
gamma <- 0.9
terminal_state <- n_states

# Environment: transition and reward models
set.seed(42)
transition_model <- array(0, dim = c(n_states, n_actions, n_states))
reward_model <- array(0, dim = c(n_states, n_actions, n_states))

for (s in 1:(n_states - 1)) {
  transition_model[s, 1, s + 1] <- 0.9
  transition_model[s, 1, sample(1:n_states, 1)] <- 0.1
  transition_model[s, 2, sample(1:n_states, 1)] <- 0.8
  transition_model[s, 2, sample(1:n_states, 1)] <- 0.2
  for (s_prime in 1:n_states) {
    reward_model[s, 1, s_prime] <- ifelse(s_prime == n_states, 1.0, 0.1 * runif(1))
    reward_model[s, 2, s_prime] <- ifelse(s_prime == n_states, 0.5, 0.05 * runif(1))
  }
}

transition_model[n_states, , ] <- 0
reward_model[n_states, , ] <- 0

# Sampling function
sample_env <- function(s, a) {
  probs <- transition_model[s, a, ]
  s_prime <- sample(1:n_states, 1, prob = probs)
  reward <- reward_model[s, a, s_prime]
  list(s_prime = s_prime, reward = reward)
}
```

## Step 2: Off-Policy Q-Learning Implementation in R

Q-learning uses an ε-greedy behavior policy but learns the greedy target policy via TD updates.

```r
off_policy_q_learning <- function(episodes = 1000, alpha = 0.1, epsilon = 0.1) {
  Q <- matrix(0, nrow = n_states, ncol = n_actions)
  
  for (ep in 1:episodes) {
    s <- 1
    while (TRUE) {
      a <- if (runif(1) < epsilon) sample(1:n_actions, 1) else which.max(Q[s, ])
      out <- sample_env(s, a)
      s_prime <- out$s_prime
      reward <- out$reward
      
      Q[s, a] <- Q[s, a] + alpha * (reward + gamma * max(Q[s_prime, ]) - Q[s, a])
      
      if (s_prime == n_states) break
      s <- s_prime
    }
  }
  list(Q = Q, policy = apply(Q, 1, which.max))
}
```

Run Q-learning:

```r
ql_result <- off_policy_q_learning()
ql_policy_before <- ql_result$policy
```

## Step 3: Off-Policy Monte Carlo Implementation in R

Off-policy MC uses a random behavior policy and importance sampling to learn the greedy target policy.

```r
off_policy_monte_carlo <- function(episodes = 1000, alpha = 0.1, epsilon = 0.1) {
  Q <- matrix(0, nrow = n_states, ncol = n_actions)
  C <- matrix(0, nrow = n_states, ncol = n_actions)  # Cumulative importance weights
  
  for (ep in 1:episodes) {
    episode <- list()
    s <- 1
    # Use random behavior policy (uniform over actions)
    while (TRUE) {
      a <- sample(1:n_actions, 1)
      out <- sample_env(s, a)
      episode[[length(episode) + 1]] <- list(state = s, action = a, reward = out$reward)
      if (out$s_prime == n_states) break
      s <- out$s_prime
    }
    
    G <- 0
    W <- 1  # Importance sampling ratio
    for (t in length(episode):1) {
      s <- episode[[t]]$state
      a <- episode[[t]]$action
      r <- episode[[t]]$reward
      G <- gamma * G + r
      
      # Update only if the action matches the greedy target policy
      greedy_a <- which.max(Q[s, ])
      if (a == greedy_a) {
        C[s, a] <- C[s, a] + W
        Q[s, a] <- Q[s, a] + (W / C[s, a]) * (G - Q[s, a])
      }
      
      # Update importance sampling ratio (behavior: uniform, target: greedy)
      W <- W * (ifelse(a == greedy_a, 1 / epsilon, 0) / (1 / n_actions))
      if (W == 0) break  # Early termination if ratio becomes zero
    }
  }
  list(Q = Q, policy = apply(Q, 1, which.max))
}
```

Run off-policy MC:

```r
mc_result <- off_policy_monte_carlo()
mc_policy_before <- mc_result$policy
```

## Step 4: Simulating Reward Devaluation

We devalue the terminal state’s reward to zero to simulate environmental change.

```r
# Devalue terminal reward
for (s in 1:(n_states - 1)) {
  reward_model[s, 1, n_states] <- 0
  reward_model[s, 2, n_states] <- 0
}
```

## Step 5: Comparing Policies Before and After Devaluation

Without retraining, off-policy methods retain their original policies, reflecting habitual behavior.

```r
# Off-policy methods keep previous policies (habitual)
ql_policy_after <- ql_policy_before
mc_policy_after <- mc_policy_before
```

## Step 6: Visualizing the Policies

Visualize the policies to compare behavior.

```r
plot_policy <- function(policy, label, col = "skyblue") {
  barplot(policy, names.arg = 1:n_states, col = col,
          ylim = c(0, 3), ylab = "Action (1=A1, 2=A2)",
          main = label)
  abline(h = 1.5, lty = 2, col = "gray")
}

par(mfrow = c(2, 2))

plot_policy(ql_policy_before, "Q-Learning Policy Before", "orange")
plot_policy(ql_policy_after, "Q-Learning Policy After", "orange")
plot_policy(mc_policy_before, "Off-Policy MC Policy Before", "orchid")
plot_policy(mc_policy_after, "Off-Policy MC Policy After", "orchid")
```

## Interpretation and Discussion

Q-learning and off-policy Monte Carlo learn the optimal (greedy) policy from exploratory behavior, leveraging the flexibility of off-policy learning. Q-learning’s TD updates are efficient for online learning, while off-policy MC’s importance sampling enables learning from complete episodes, suitable for offline settings. After reward devaluation, both methods retain their policies without retraining, demonstrating habitual behavior typical of model-free RL. This contrasts with model-based methods, which adapt instantly to reward changes. Off-policy learning’s ability to use diverse data makes it valuable for applications like robotics (learning from demonstration) or recommendation systems (optimizing from user logs), though it faces challenges like variance in MC or instability in TD with function approximation.

## Conclusion

Off-policy learning enhances TD and MC methods by enabling agents to learn optimal policies from diverse, exploratory data. Q-learning’s incremental updates suit dynamic environments, while off-policy MC’s importance sampling supports offline learning. Future posts will explore off-policy learning in deep RL (e.g., DQN) and advanced methods like DDPG and SAC for continuous action spaces.

## Comparison Table

| **Aspect**                     | **Off-Policy Monte Carlo**                                                                 | **Off-Policy Temporal Difference (Q-Learning)**                                              |
|-------------------------------|-------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------|
| **Learning Approach**          | Learns from complete episodes, using importance sampling to adjust for behavior policy.     | Learns incrementally after each transition using bootstrapping.                              |
| **Update Rule**               | $Q(s,a) \leftarrow Q(s,a) + \alpha \left( \rho_t G_t - Q(s,a) \right)$                    | $Q(s,a) \leftarrow Q(s,a) + \alpha \left( r + \gamma \max_{a'} Q(s', a') - Q(s,a) \right)$ |
| **Episode Requirement**        | Requires complete episodes to compute returns and importance weights.                     | Updates online after each step, no episode completion needed.                                |
| **Bias and Variance**         | Unbiased but high variance due to importance sampling ratios.                             | Biased due to bootstrapping, lower variance.                                                |
| **Policy Type**               | Off-policy; learns greedy policy from exploratory behavior using importance sampling.      | Off-policy; learns greedy policy via max Q-value.                                           |
| **Computational Efficiency**   | Less efficient; requires episode completion and ratio calculations.                       | More efficient; updates immediately after each transition.                                  |
| **Adaptation to Change**       | Slow to adapt without retraining, relies on past episode returns.                         | Slow to adapt without retraining, but incremental updates allow faster response.             |
| **Implementation in Code**     | Stores episodes, computes weighted returns backward using importance sampling.            | Updates Q-values online using TD rule and max Q-value.                                      |
| **Example in Provided Code**   | Off-policy MC with random behavior policy, greedy target policy.                          | Q-Learning with ε-greedy behavior policy, greedy target policy.                             |
