
Machine learning has empowered intelligent computer systems to autonomously tackle tasks, contributing significantly to industrial innovation. Through the integration of high-performance computing, modern modeling, and simulations, machine learning has become an indispensable tool for managing and analyzing massive volumes of data. Despite the current perception of artificial intelligence entering a golden age, challenges persist in its development and application, necessitating solutions for unlocking its full transformative potential across industries.

While machine learning has proven its capability to handle various tasks, it does not consistently provide optimal solutions. Challenges in the field persist, emphasizing the need for continuous efforts to address obstacles hindering the technology's progress. One of the critical challenges lies in data quality, where subpar data can lead to incorrect or imprecise predictions. Additionally, data scarcity poses a significant issue, either due to insufficient available datasets or the excessive cost of manual labeling. The ethical concerns of data privacy and fairness further complicate matters, requiring a balanced and secure approach to the development and deployment of machine learning technologies.

Synthetic data, defined as artificially annotated information generated by computer algorithms or simulations, emerges as a solution to certain challenges in machine learning. This approach becomes particularly relevant when real data is either unavailable or must be kept private due to privacy or compliance risks. The use of synthetic data is pervasive across various sectors, including healthcare, business, manufacturing, and agriculture, with its demand growing exponentially. However, the ethical and secure deployment of synthetic data is crucial to mitigate potential risks associated with privacy and bias, ensuring a responsible and effective integration of this technology in practice.








The utilization of Electronic Health Record (EHR) data, encompassing clinical symptoms, diagnoses, investigations, and treatments, has gained prominence in data mining and machine learning applications. Notable instances include studies by Khalid et al., offering insights into drug use patterns, and Ravizza et al., proposing diagnostic and predictive disease applications. However, a need arises for synthetic datasets to complement real-world data due to challenges such as stringent access regulations, cost inefficiency, and privacy concerns associated with actual patient records.

Accessing individual real-world records, even in pseudonymized form, is subject to strict regulations to prevent inadvertent patient reidentification. Synthetic datasets, devoid of personal identifiers, could streamline data access approvals and accelerate research innovation. The cost-effectiveness of using synthetic data for benchmarking and validation is emphasized, offering a more economical alternative to expanding the coverage of real-world data. Additionally, the efficiency of testing algorithms or functions is highlighted, particularly for scenarios like assessing scalability and robustness.

Patient privacy protection, completeness in data research, and benchmarking capabilities are underscored as advantages of synthetic data. Despite these benefits, there is a lack of a comprehensive synthetic data generation approach for healthcare data that ensures preservation of key characteristics while safeguarding patient privacy. The proposed framework in this article aims to address this gap by focusing on clinically meaningful synthetic data generation while adhering to key requirements such as preserving biological relationships, univariate and multivariate distances, and privacy preservation.


Research in synthetic data generation with a focus on preserving data privacy has garnered attention alongside studies addressing the complexities inherent in healthcare data. This section reviews prior work in these domains to distill lessons applicable to the proposed framework.

Synthetic data generation methods broadly fall into three categories. The first group involves generating synthetic data based on statistical properties of real-world data. This is particularly valuable when real data are challenging to access or exhibit scarcity, as seen in studies sampling data from population distributions or using statistical information and expert knowledge to simulate care patterns. The second group involves adding noise to regenerate part of the real-world data, useful for scenarios such as data imputation to address missing values. The third group utilizes machine learning techniques to generate or extend datasets through prediction and inference, capable of producing both semi-synthetic and fully synthetic data. However, the longitudinal nature and complex format of healthcare data pose additional considerations when employing these approaches.

Privacy preservation, particularly with the introduction of the General Data Protection Regulation (GDPR), is a critical concern. Various approaches, such as perturbation, condensation, randomization, and fuzzy-based methods, have been proposed to mitigate the risk of patient privacy disclosure. Nevertheless, anonymization may compromise data utility, and even anonymized data might carry residual privacy risks, as demonstrated in studies revealing the uniqueness of individuals based on seemingly innocuous information. Fully synthetic datasets offer a potential solution to privacy concerns by ensuring all data values differ from real-world data. However, in clinical studies, relying on a restricted set of values may be necessary to maintain clinical meaningfulness, with the caveat that privacy risks persist if synthetic outliers align with those in real-world data by chance.

The complexity in electronic healthcare data stems from heterogeneous data sources and intricate relationships within the data. Heterogeneity refers to varied data representations, such as cross-sectional versus longitudinal data, with the conversion between these formats posing a processing challenge. Relationships within the data model, involving entities like patient, clinical observation, and drug records, contribute to complexity. Comprehensive data models, as exemplified by multidimensional models, aim to address the intricate relationships, but learning these relationships remains a challenge to retain clinical information fidelity.



**A proposed synthetic data framework** aims to facilitate the synthesis of healthcare data with a focus on maintaining a high degree of similarity to the ground truth, encompassing biological relationships and values, while also safeguarding privacy. The framework delineates several processes, including ground truth selection, synthetic data generation, evaluation, and the iterative selection of sensible synthetic data.

The ground truth selection process involves two intertwined tasks: identifying privacy-sensitive variables and defining biological relationships. The results of these tasks inform the study data model, guiding the choice of a suitable synthetic data generation model. The synthetic data generation process aims to produce synthetic data candidates while ensuring data quality in terms of consistent data values and format. The subsequent evaluation process focuses on assessing key criteria, including biological relationship preservation, univariate and multivariate distances, and privacy preservation. The sensible synthetic data selection is an iterative process that concludes when all evaluation criteria are met.

In the ground truth selection process, variables are chosen based on their relevance to the study context. Privacy-sensitive variable identification involves considering combinations of variables that might pose a risk of reidentification. Reference to guidance from entities like the UK's Information Commissioner's Office (ICO) can aid in predefining a set of sensitive variables. Additionally, well-established biological relationships between variables and the study target must be defined at this stage.

Considerations for the synthetic data generation process include the identification of biological relationships, the privacy-sensitive nature of variables, and the number of variables. Adding noise can be effective when full synthetic data are not needed, but balancing privacy risk with data utility is crucial. Expert-driven approaches and machine learning algorithms offer solutions based on factors such as data accessibility, complexity, and transparency requirements. Transparency becomes vital, especially when interpreting the model architecture and results of machine learning algorithms. A recommendation emphasizes considering more transparent algorithms, such as Bayesian networks, when alternatives are available. After generating synthetic data, a data quality check may be necessary for variables with continuous values to ensure biological plausibility.



In the evaluation process, synthetic data generation models produce multiple candidates, necessitating a method to distinguish between them. The evaluation relies on measuring the "distance" to quantify the disparity between the synthetic dataset and the ground truth, denoted as dis(Xs, Xg), where Xs and Xg represent the input spaces of synthetic data and ground truth, respectively. Generally, a smaller distance indicates greater similarity between the two datasets.

Within the univariate distance assessment, discrete variables (D) and continuous variables (C) are considered. For discrete variables, the probability distribution p represents individual variables, while the density function f represents continuous variables. Univariate distance is defined using equations (E1) and (E2), where each variable is independently assessed, and the dis function remains consistent for a variable type. Probability difference and hypothesis test results, such as p-values from tests like the Kolmogorov–Smirnov test (KS test), are utilized as dis functions.

Multivariate distance analysis is employed to compare the interrelationship and patterns of data instances in synthetic and ground truth datasets. The dis function in multivariate distance measures the Euclidean distance in a transformed input space X ∈ ℝk, where the n dimensions can be reduced and projected to shared k dimensions. Various multivariate analysis methods, including nonmetric multidimensional scaling (NMDS), are utilized for this purpose, enabling a comprehensive assessment of the similarities between synthetic and ground truth datasets.


Evaluating the quality of synthetic medical data is an active research focus, often categorized into three key qualities: fidelity, diversity, and generalization. Fidelity assesses the quality of samples, questioning their distinguishability from real samples and the validity of population inferences made from synthetic ones. Utility, as a narrow evaluation of fidelity, gauges the validity of inferences, and fidelity metrics can be either computational or involve human evaluation, utilizing distances between real and synthetic data distributions or expert assessments. Diversity measures the coverage of the real data population, addressing potential subgroup underrepresentation, while generalization pertains to privacy concerns, examining whether synthetic data samples are replicas of real data. Privacy assessment metrics come in empirical (through privacy attacks) and formal (derived from the generation method) categories, and stakeholders must determine the balance among these qualities based on their priorities and objectives.

## References

https://onlinelibrary.wiley.com/doi/full/10.1111/coin.12427

https://www.sciencedirect.com/science/article/pii/S2589004222016030

https://www.zotero.org/groups/5185601/synthetic_data_whitepaper/collections/EARWRPS6/items/YUUMDYNX/collection

https://arxiv.org/abs/2302.04062


