---
layout: post
categories: posts
title: Responsible AI and Supervised fine-tuning (SFT)
featured-image: /images/RAI.png
tags: [Artificial intelligence, Responsible AI, SFT]
date-string: November 2023
---

###  Supervised fine-tuning (SFT)
Supervised fine-tuning (SFT) is a technique used in machine learning, specifically in the field of transfer learning that involves taking a pre-trained model, typically a deep neural network, that has been trained on a large dataset for a related task, and then further training it on a smaller labeled dataset specific to the task at hand. The idea is to leverage the knowledge and representations learned by a model on a source task and apply it to a target task. The pre-trained model serves as a starting point, providing a good initialization for the target task. However, since the pre-trained model is trained on a different task or dataset, it may not directly fit the target task's data. This is where supervised fine-tuning comes into play. The pre-trained model's parameters are further adjusted or fine-tuned using the labeled data from the target task. During fine-tuning, the model's weights are updated using backpropagation and gradient descent, optimizing the model's performance specifically for the target task. By fine-tuning the pre-trained model, it becomes more tailored and adapted to the specific characteristics and nuances of the target task's data. Supervised fine-tuning is particularly useful when the target task has a smaller labeled dataset compared to the original pre-training dataset. Instead of training a model from scratch on the target task, which may require a larger amount of labeled data, SFT allows for efficient re-use of the knowledge already captured by the pre-trained model. This can significantly reduce the training time and resource requirements for the target task while still achieving good performance.


### Alignment and Responsible AI through SFT

SFT offers a pathway towards achieving alignment between human values and the behavior of AI systems. As AI continues to advance and play an increasingly prominent role in our lives, ensuring alignment becomes a critical objective. SFT presents a promising approach to bridge the gap between pre-trained models and specific tasks, allowing for customization and alignment with human preferences. Here we will delve into the concept of SFT, its underlying principles, and its potential as a technique toward achieving alignment. At its core, SFT involves taking a pre-trained model, typically trained on a large dataset, and fine-tuning it on a more specific task using labeled data. The pre-training phase equips the model with a general understanding of various concepts, while the fine-tuning phase refines its performance for the task at hand. This two-step process enables the model to leverage existing knowledge and adapt it to the specific requirements and nuances of a particular task, facilitating alignment with human values and objectives. One of the key advantages of SFT is its ability to incorporate human supervision during the fine-tuning process. By providing labeled data and guidance, human experts can shape the behavior of the AI system to align with desired outcomes. This human feedback serves as a crucial mechanism for correcting biases, refining decision-making, and incorporating ethical considerations. By utilizing labeled data, the fine-tuning process becomes more transparent, as the model's behavior can be linked to specific examples and human annotations. This transparency enables stakeholders to understand and assess the decision-making process of the AI system. Furthermore, the interpretability of SFT allows for introspection and validation of the model's alignment with human values, ensuring that its behavior is accountable and explainable.

While SFT presents a promising technique toward achieving alignment, it is important to acknowledge its challenges and limitations. The quality and representativeness of the labeled data used for fine-tuning significantly impact the alignment achieved. Biases or inaccuracies in the labeled data can propagate into the fine-tuned model, potentially leading to misalignment. Additionally, the process of providing human supervision and feedback during fine-tuning requires expertise and resources, which may not always be readily available. To overcome these challenges, ongoing research focuses on developing techniques that improve the quality and diversity of labeled data. Active learning approaches aim to intelligently select data points for labeling, maximizing the information gained while minimizing the need for extensive labeling efforts. Adversarial fine-tuning techniques seek to identify and mitigate biases introduced during the fine-tuning process, promoting fairness and alignment. These advancements contribute to the ongoing refinement of SFT and its potential to achieve greater alignment between AI systems and human values. SFT is related to responsible AI in the context of adapting pre-trained models to specific tasks while considering ethical and responsible considerationsFor instance when applying SFT, one has to ensure that the pre-trained model used for fine-tuning is itself fair and unbiased and to avoid reinforcing or amplifying any biases present in the pre-trained model during the fine-tuning process. Additionally, data used for fine-tuning should be carefully selected and representative to mitigate biases in the resulting model. Liewise, with SFT, the adapted model should be continuously evaluated to assess its performance, including monitoring for biases, fairness, and unintended consequences. Feedback mechanisms should be established to gather insights from users and stakeholders, enabling iterative improvements and addressing any ethical concerns that arise during the deployment of the fine-tuned model.  It is important to establish clear ownership and responsibility for the fine-tuning process, including monitoring and evaluating the impact of the adapted model to detect and address any unintended consequences or ethical issues.

###  Supervised fine-tuning (SFT) and Reinforcement Learning from Human Feedback (RLHF)

Supervised fine-tuning (SFT) and Reinforcement Learning from Human Feedback (RLHF) are different approaches to improve machine learning models. The main objective of SFT is to adapt a pre-trained model to a specific target task by further training it on a labeled dataset. The focus is on achieving high performance on the target task by leveraging knowledge from the pre-training. RLHF, on the other hand, aims to improve an model's decision-making through interaction with human feedback, whether it be explicit rewards or evaluations. Morevoer, SFT follows a supervised learning paradigm, where the model is trained on labeled examples with a well-defined loss function. It leverages the labeled data to update the model's weights and typically requires a relatively large labeled dataset specific to the target task. The pre-trained model serves as a starting point and requires further training on this labeled data. RLHF, follows a reinforcement learning paradigm, where the agent interacts with an environment and learns from feedback signals, typically in the form of rewards or evaluations. It employs exploration-exploitation strategies to optimize its policy. It can learn from sparse or even noisy feedback, as long as it provides sufficient information for the agent to improve its decision-making. RLHF can potentially learn from a smaller amount of human feedback, which can be more easily obtained compared to large labeled datasets. In the context of LMMs,the RLHF framework is generic and can be used to optimize an LLM based on a variety of different objectives using a unified approach. RLHF can be used to transform generic, pre-trained LLMs into the impressive information-seeking dialogue agents that we commonly see today (e.g., ChatGPT). SFT in contrast, is a method to adapt a pre-trained model to a specific target task by fine-tuning it on labeled data, with a focus on achieving high performance on the target task. 


### Healthcare

SFT finds valuable applications in the healthcare field, enhancing the performance of AI models across various domains. In medical image analysis, SFT enables the fine-tuning of pre-trained deep learning models for tasks like image segmentation, object detection, and classification. This approach allows models to learn specific medical features and patterns, leading to improved accuracy in disease diagnosis, abnormality detection, and treatment planning. Another area where SFT proves beneficial is in the analysis of electronic health records (EHRs). By fine-tuning language models using labeled EHR data, SFT aids in extracting relevant information from unstructured clinical text, facilitating tasks such as identifying medical conditions, predicting patient outcomes, and supporting clinical decision-making. In the era of telemedicine and remote patient monitoring, SFT finds application in analyzing patient data collected through wearable devices, sensors, and remote monitoring systems. SFT could enable accurate detection of abnormalities, early warning signs, and personalized healthcare recommendations to enhance remote patient care and enables more effective telemedicine practices. These examples illustrate the broad potential of SFT in healthcare, where it assists in improving the accuracy, efficiency, and effectiveness of AI models. By leveraging supervised fine-tuning techniques, healthcare providers can harness the power of AI to support diagnostics, treatment planning, drug discovery, clinical decision-making, and remote patient monitoring.

### Downsides

Although SFT offers valuable advantages, it is important to consider its potential downsides. Fine-tuning large-scale models can be computationally intensive, necessitating substantial computing infrastructure and time. Another limitation is the reliance on labeled data for the fine-tuning process. Obtaining high-quality labeled data can be time-consuming, costly, and resource-intensive. For instance in healthcare, acquiring labeled data that accurately represents the complex and diverse nature of medical conditions and patient populations can be particularly challenging. Insufficient or biased labeled data may lead to suboptimal fine-tuning results and impact the generalizability of the model's performance. Moreover in the context of healthcare the use of labeled data containing sensitive patient information raises concerns about privacy and data protection. Safeguarding patient privacy and complying with relevant data protection regulations is crucial to maintain trust in healthcare AI applications. Additionally, the potential bias in the labeled data used for fine-tuning can introduce ethical challenges. Biases in the data, such as disparities in healthcare access or underrepresentation of certain demographic groups, can be perpetuated and amplified by the fine-tuned models, leading to inequitable outcomes and exacerbating healthcare disparities.

### Future of SFT

SFT holds a promising future across various domains with several key aspects that shape its potential in the coming years. Researchers are continually refining fine-tuning techniques, exploring novel architectures, and optimizing hyperparameters to achieve better results. As models become more sophisticated and datasets improve in size and quality, SFT will likely lead to even more accurate and effective AI systems. By leveraging pre-trained models as a foundation for fine-tuning, SFT enables the transfer of knowledge from one domain to another. Future advancements in transfer learning approaches will enable models to adapt more efficiently to new tasks and domains. This enhanced generalization capability will reduce the need for extensive retraining and accelerate the deployment of AI systems in real-world applications. Moreover, techniques such as few-shot learning, meta-learning, and active learning will reduce the reliance on large labeled datasets. This will expand the applicability of SFT to domains with limited labeled data availability, making AI models more practical and accessible. The future of SFT will focus on interpretability and explainability, with models grow in complexity, the ability to interpret and explain their decisions becomes crucial. Researchers will develop techniques that provide transparent explanations for the behavior of fine-tuned models to understand and validate the decisions made by the models, fostering trust and acceptance. Ensuring the robustness and safety of fine-tuned models rest a critical concern and researchers will explore techniques to enhance the models' resilience against adversarial attacks, spurious correlations, and unforeseen situations. Incorporating mechanisms for uncertainty estimation and risk assessment will contribute to the development of more reliable and secure AI systems.


### References

+ [Power of Supervised Finetuning with Open Source Large Language Models(LLMs)](https://medium.datadriveninvestor.com/lima-efficient-large-language-model-with-supervised-finetuning-bad42f7a48a6)
+ Holstein, K., Wortman Vaughan, J., Daumé III, H., Dudík, M., & Wallach, H. (2019). Improving fairness in machine learning systems: What do industry practitioners need? In Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems (pp. 1-16). doi: 10.1145/3290605.3300830
+ Mitchell, M., Wu, S., Zaldivar, A., Barnes, P., Vasserman, L., Hutchinson, B., ... & Gebru, T. (2019). Model cards for model reporting. In Proceedings of the Conference on Fairness, Accountability, and Transparency (pp. 220-229). doi: 10.1145/3287560.3287596
+ Arrieta, A. B., Díaz-Rodríguez, N., Del Ser, J., Bennetot, A., Tabik, S., Barbado, A., ... & Herrera, F. (2020). Explainable Artificial Intelligence (XAI): Concepts, taxonomies, opportunities and challenges toward responsible AI. Information Fusion, 58, 82-115. doi: 10.1016/j.inffus.2019.12.012
+ Honegger, A., & Passweg, D. (2021). Supervised fine-tuning for controlled and responsible AI. arXiv preprint arXiv:2106.11539.
+ Jobin, A., Ienca, M., & Vayena, E. (2019). The global landscape of AI ethics guidelines. Nature Machine Intelligence, 1(9), 389-399. doi: 10.1038/s42256-019-0088-2
+ Barredo Arrieta, A., Díaz-Rodríguez, N., Del Ser, J., Bennetot, A., Tabik, S., Barbado, A., ... & Herrera, F. (2020). Explainable Artificial Intelligence (XAI): Concepts, taxonomies, opportunities and challenges toward responsible AI. Information Fusion, 58, 82-115. doi: 10.1016/j.inffus.2019.12.012
+ Campolo, A., Sanfilippo, M., Whittaker, M., & Crawford, K. (2017). AI Now 2017 Report. AI Now Institute at New York University. Retrieved from https://ainowinstitute.org/AI_Now_2017_Report.pdf
+ Floridi, L., & Cowls, J. (2019). A unified framework of five principles for AI in society. Harvard Data Science Review, 1(1). doi: 10.1162/99608f92.8cd550d1
