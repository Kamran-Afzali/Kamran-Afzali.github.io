
# Causal Inference in Practice: Instrumental Variables with a Healthcare Application

## Introduction

In observational studies, establishing causal relationships is often challenging due to confounding—situations where unobserved variables influence both the treatment assignment and the outcome, leading to biased estimates. While methods like Difference-in-Differences address specific types of confounding, they may not always suffice. Instrumental Variables (IV) provide a powerful econometric method to estimate causal effects in the presence of unmeasured confounding, particularly when a direct randomized controlled trial is not feasible or ethical. IV leverages a specific type of variable—the instrumental variable—that influences the treatment but affects the outcome only through the treatment, thereby allowing for the isolation of the causal effect.

This post will delve into the Instrumental Variables method, exploring its theoretical underpinnings, mathematical formalism, and practical application. We will use a realistic healthcare example—estimating the causal impact of a specific medical procedure on patient recovery—to demonstrate the methodology using R code. We will also discuss the crucial assumptions, diagnostic checks, and limitations to ensure robust causal inference.

## Instrumental Variables: Theoretical Framework

### Motivation and Identification

Instrumental Variables (IV) methods offer a strategy for causal inference in the presence of **unmeasured confounding**, a situation where the standard assumption of conditional ignorability fails. That is, when potential outcomes $Y(1)$, $Y(0)$ are not conditionally independent of treatment $D$ given observed covariates $X$, alternative identification strategies are required. IV methods achieve this by exploiting exogenous variation in treatment induced by an external variable, known as an **instrument** $Z$, which must satisfy a set of well-defined assumptions.

The validity of an instrumental variable relies on the following three core conditions:

1.  **Relevance**: The instrument must have a non-zero correlation with the treatment variable. Formally,

    $$
    $$$$\\operatorname{Cov}(Z, D) \\neq 0,

    $$
    $$$$which ensures that the instrument induces variation in the treatment.

2.  **Exclusion Restriction**: The instrument must affect the outcome $Y$ only through its effect on the treatment $D$. That is, conditional on the treatment and covariates, the instrument has no direct effect on the outcome:

    $$
    $$$$Y = f(D, X, \\varepsilon), \\quad \\text{with } Z \\notin f.

    $$
    $$$$
    $$
3.  **Independence (or Exogeneity)**: The instrument must be independent of the unobserved determinants of the outcome, i.e., the instrument is as good as randomly assigned:

    $$
    $$$$Z \\perp {Y(1), Y(0)} \\mid X.

    $$
    $$$$
    $$When these conditions are met, the IV approach identifies the **Local Average Treatment Effect (LATE)**—the average treatment effect for the subpopulation of **compliers**, individuals whose treatment status is influenced by the instrument. It is important to emphasize that LATE may differ from the Average Treatment Effect (ATE), as it pertains only to a specific subpopulation determined by the instrument’s influence.

### Theoretical Estimation Framework

In the case of binary instruments and binary treatments, the IV estimand can be expressed via the **Wald estimator**, given by:

$$\widehat{\text{LATE}} = \frac{\mathbb{E}[Y \mid Z = 1] - \mathbb{E}[Y \mid Z = 0]}{\mathbb{E}[D \mid Z = 1] - \mathbb{E}[D \mid Z = 0]}.$$

This ratio captures the causal effect among compliers by scaling the change in outcomes induced by the instrument by the change in treatment uptake.

In more general settings, particularly with continuous instruments or treatments and with covariates $X$, the causal effect is commonly estimated via **two-stage least squares (2SLS)**. The procedure consists of two sequential regressions:

  * **First stage**: Estimate the predicted treatment value $\\hat{D}\_i$ using the instrument:

    $$
    $$$$D\_i = \\pi\_0 + \\pi\_1 Z\_i + \\pi\_2^\\top X\_i + \\nu\_i.

    $$
    $$$$
    $$
  * **Second stage**: Regress the outcome $Y\_i$ on the predicted treatment:

    $$
    $$$$Y\_i = \\alpha\_0 + \\alpha\_1 \\hat{D}\_i + \\alpha\_2^\\top X\_i + \\varepsilon\_i.

    $$
    $$$$
    $$The coefficient $\\alpha\_1$ from the second stage provides a consistent estimate of the causal effect under the IV assumptions. Importantly, this estimator remains consistent even in the presence of omitted variable bias, provided the instrument is valid.

Instrumental Variables (IV) estimation differs substantially from other causal inference strategies, such as **propensity score methods** and **Difference-in-Differences (DiD)**, both in terms of identifying assumptions and target estimands.

Propensity score matching and related methods (e.g., inverse probability weighting, doubly robust estimators) rely on the **conditional ignorability assumption**, i.e., $Y(1), Y(0) \\perp D \\mid X$, which requires that all confounders influencing both treatment and outcome are observed and properly adjusted for. These approaches target the **Average Treatment Effect (ATE)** or the **Average Treatment effect on the Treated (ATT)** across the full population or a matched subpopulation. In contrast, IV methods do not require all confounders to be measured but instead depend on the presence of a valid instrument and identify LATE, a more restricted estimand.

Difference-in-Differences (DiD) methods exploit temporal variation in treatment exposure across groups and require the **parallel trends assumption**: that, in the absence of treatment, the difference in outcomes between treated and control groups would have remained constant over time. DiD can accommodate some unobserved confounding, provided it is time-invariant. However, DiD assumes the absence of differential trends, an assumption that cannot be tested directly. Unlike IV, DiD does not rely on an instrument but on quasi-experimental variation in timing or assignment. IV estimation offers a powerful alternative when unmeasured confounding invalidates conditional ignorability, but it comes with its own limitations, such as the difficulty of finding valid instruments and the interpretation of effects limited to compliers. While DiD and propensity score methods may estimate broader effects under stronger assumptions, IV provides robustness to hidden bias at the cost of narrower causal generalizability.

### Practical Considerations

  - **Finding a Valid Instrument**: Identifying a strong and valid instrument is often the most challenging aspect of IV. It requires deep institutional knowledge and careful consideration of potential pathways.
  - **Multiple Instruments**: When multiple instruments are available, overidentification tests (e.g., Sargan-Hansen test) can assess the validity of the instruments as a group, assuming at least one instrument is valid.
  - **Software Implementation**: Specialized packages in R (e.g., `AER`, `ivreg`) and Python (e.g., `statsmodels`) are available to perform IV regression and associated diagnostics.

## Application: Estimating the Impact of a Medical Procedure on Recovery Time

### Scenario

Consider a scenario where we want to estimate the causal effect of a specific surgical procedure (e.g., a minimally invasive technique vs. traditional open surgery) on patient recovery time (e.g., length of hospital stay). A direct comparison using observational data might be confounded by patient severity, surgeon skill, or hospital resources—unobserved factors that influence both the choice of procedure and recovery.

Let's assume that **distance to a specialized surgical center** acts as an instrumental variable. Patients living closer to the specialized center are more likely to undergo the minimally invasive procedure due to convenience and access, but the distance itself does not directly affect recovery time, only indirectly through its influence on the procedure choice.

### Simulated Data

We simulate data for 500 patients, including their recovery time (in days), whether they underwent the minimally invasive procedure, their distance to the specialized center, and a confounding variable (e.g., initial health status, which is unobserved to the analyst). The true causal effect of the minimally invasive procedure is assumed to reduce recovery time by 5 days.

### R Implementation

Below is the R code to simulate the data, estimate the IV effect using 2SLS, and perform basic diagnostics.

```r
# Load required packages
library(AER) # For ivreg function
library(dplyr)
library(ggplot2)

# Set seed for reproducibility
set.seed(456)

# Parameters
n_patients <- 500
true_effect_procedure <- -5 # Minimally invasive procedure reduces recovery by 5 days

# Simulate data
data <- data.frame(
  patient_id = 1:n_patients,
  distance_to_center = runif(n_patients, min = 10, max = 100), # Instrument Z
  unobserved_confounder = rnorm(n_patients, mean = 0, sd = 2), # Unobserved confounder
  # Procedure (endogenous treatment D) influenced by distance and unobserved confounder
  # Logit model for binary treatment: P(procedure=1) = f(distance, unobserved_confounder)
  procedure_propensity = -0.05 * runif(1, min = 0.5, max = 1.5) * data$distance_to_center +
                         1 * data$unobserved_confounder + rnorm(n_patients, 0, 1),
  # Recovery time (outcome Y) influenced by procedure, unobserved confounder, and noise
  recovery_base = 20 + # Baseline recovery
                  1.5 * data$unobserved_confounder + # Effect of confounder on recovery
                  rnorm(n_patients, mean = 0, sd = 3) # Random noise
)

# Convert propensity to binary procedure choice
data$procedure <- as.numeric(data$procedure_propensity > mean(data$procedure_propensity))

# Add the true treatment effect to recovery time
data$recovery_time <- data$recovery_base + true_effect_procedure * data$procedure

# Preview data
head(data)

# --- Instrumental Variables (2SLS) Regression ---

# First stage: Regress endogenous treatment (procedure) on instrument (distance_to_center)
first_stage <- lm(procedure ~ distance_to_center, data = data)
summary(first_stage)

# Check relevance: F-statistic from first stage
# (For a single instrument, this is simply the t-stat squared for the instrument)

# Get predicted values of the procedure
data$predicted_procedure <- predict(first_stage)

# Second stage: Regress outcome (recovery_time) on predicted treatment
second_stage <- lm(recovery_time ~ predicted_procedure, data = data)
summary(second_stage)

# Alternative using ivreg from AER package (recommended for robust inference)
# Formula: outcome ~ endogenous_variables | instruments + exogenous_variables
iv_model <- ivreg(recovery_time ~ procedure | distance_to_center, data = data)
summary(iv_model)

# Manual IV estimator (simple case with one instrument, one endogenous variable)
# Cor(Y,Z) / Cor(D,Z)
manual_iv_estimate <- cov(data$recovery_time, data$distance_to_center) / cov(data$procedure, data$distance_to_center)
cat("Manual IV Estimate:", manual_iv_estimate, "\n")

# Interpretation of relevance: Check first stage F-statistic for strong instrument
# The summary(first_stage) above provides the F-statistic.
# A high F-statistic (e.g., >10) suggests a strong instrument.
```

### Interpretation

  - **Relevance (First Stage)**: The `summary(first_stage)` output provides the F-statistic for the instrument (`distance_to_center`). A significant F-statistic (e.g., \>10 for a rule of thumb) indicates that the instrument is sufficiently correlated with the endogenous treatment, fulfilling the relevance assumption. In our simulation, we would expect a strong negative correlation, meaning shorter distances lead to a higher probability of undergoing the minimally invasive procedure.
  - **IV Estimate (Second Stage / `ivreg`)**: The coefficient on `predicted_procedure` in the `summary(second_stage)` or on `procedure` in the `summary(iv_model)` output represents the estimated causal effect of the minimally invasive procedure on recovery time. This estimate should be close to our true effect of -5 days, indicating a reduction in recovery time.
  - **Exogeneity and Exclusion Restriction**: These assumptions are not statistically testable from the data alone. They must be justified by strong theoretical arguments and domain knowledge. In our scenario, we argue that the **distance to the specialized center** only affects recovery time *because* it influences the choice of procedure, and not through any other direct pathway or by being correlated with unobserved patient characteristics that also affect recovery time.

In our simulation, the estimated effect should be close to the true effect (-5 days), provided the instrument is sufficiently strong and valid.

## Limitations and Extensions

  - **Weak Instruments**: If the instrument is only weakly correlated with the endogenous treatment, IV estimates can be severely biased towards the OLS estimate and have very large standard errors. Always check the first-stage F-statistic.
  - **Violation of Exclusion Restriction**: If the instrument affects the outcome through a pathway other than the treatment, the IV estimate will be biased. This is a critical assumption requiring careful consideration.
  - **Violation of Exogeneity**: If the instrument is correlated with unobserved confounders, the IV estimate will be biased.
  - **Local Average Treatment Effect (LATE)**: The IV estimator identifies the LATE for "compliers"—those whose treatment status is influenced by the instrument. This may not be representative of the Average Treatment Effect (ATE) for the entire population.
  - **Multiple Endogenous Variables**: IV can be extended to handle multiple endogenous variables using multiple instruments, requiring more complex identification strategies.
  - **Heterogeneous Effects**: The interpretation of IV can be more complex when treatment effects are heterogeneous across individuals.

## Healthcare Example: Estimating the Causal Effect of Hospital Quality on Patient Recovery

### Context and Problem

Suppose we want to estimate the causal effect of hospital quality (treatment) on patient recovery time after surgery (outcome). For simplicity, let’s define hospital quality as a binary variable $D\_i \\in {0, 1}$, where $D\_i = 1$ indicates a patient is treated at a high-quality hospital (e.g., one with advanced facilities and experienced staff) and $D\_i = 0$ indicates a standard hospital. The outcome $Y\_i$ is the number of days until recovery, and lower values indicate faster recovery. A naive approach might compare recovery times between patients at high-quality versus standard hospitals using regression:

$$Y_i = \beta_0 + \beta_1 D_i + \beta_2 X_i + \epsilon_i$$

where $X\_i$ includes observed confounders like age, sex, and pre-existing conditions. However, unmeasured confounding—such as patients’ socioeconomic status or health consciousness—may bias $\\beta\_1$. For example, wealthier or more health-conscious patients might choose high-quality hospitals *and* recover faster due to better overall health, not hospital quality.

### Instrumental Variables Approach

To address this, we use IV estimation, leveraging an instrument $Z\_i$ that influences hospital choice but not recovery time except through hospital quality. A plausible instrument is **geographic proximity to a high-quality hospital**, defined as a binary variable $Z\_i = 1$ if the patient lives within 10 miles of a high-quality hospital and $Z\_i = 0$ otherwise. The intuition is that patients closer to a high-quality hospital are more likely to choose it due to convenience, but proximity itself does not directly affect recovery time (except through hospital choice).

The IV framework requires three assumptions, as outlined earlier:

1.  **Relevance**: Proximity must influence hospital choice:

    $$
    $$$$\\text{Cov}(Z\_i, D\_i) \\neq 0

    $$
    $$$$Patients living closer to a high-quality hospital are more likely to attend it, perhaps due to lower travel costs or familiarity. This can be tested empirically by checking if $Z\_i$ predicts $D\_i$ in a first-stage regression.

2.  **Exclusion Restriction**: Proximity affects recovery time $Y\_i$ only through hospital choice $D\_i$, not directly or via other channels. For instance, living near a high-quality hospital shouldn’t influence recovery through factors like local healthcare access, assuming similar baseline care quality across regions.

3.  **Independence**: Proximity is “as good as randomly assigned,” independent of potential outcomes $Y\_i(1)$, $Y\_i(0)$:

    $$
    $$$$Z\_i \\perp {Y\_i(1), Y\_i(0)}

    $$
    $$$$This assumes proximity is not correlated with unmeasured confounders like patient health consciousness, which holds if residential patterns are unrelated to health behaviors after controlling for observed covariates $X\_i$.

Under these assumptions, IV estimates the **Local Average Treatment Effect (LATE)**, the causal effect of high-quality hospital care on recovery time for **compliers**—patients who choose a high-quality hospital because they live nearby but would choose a standard hospital if they lived farther away.

### Formal Estimation

The IV effect can be estimated using the **Wald estimator** for binary $Z\_i$ and $D\_i$:

$$\widehat{\text{LATE}} = \frac{\mathbb{E}[Y_i \mid Z_i = 1] - \mathbb{E}[Y_i \mid Z_i = 0]}{\mathbb{E}[D_i \mid Z_i = 1] - \mathbb{E}[D_i \mid Z_i = 0]}$$

The numerator is the difference in average recovery times between patients living near versus far from a high-quality hospital (the reduced-form effect). The denominator is the difference in the probability of attending a high-quality hospital based on proximity (the first-stage effect). For example, if patients near a high-quality hospital recover 2 days faster on average, and proximity increases the likelihood of attending a high-quality hospital by 0.4, the LATE is $-2 / 0.4 = -5$ days, meaning high-quality hospitals reduce recovery time by 5 days for compliers.

Alternatively, we can use **two-stage least squares (2SLS)** for more flexibility, especially with continuous outcomes or covariates:

1.  **First Stage**: Regress hospital quality $D\_i$ on proximity $Z\_i$ and covariates $X\_i$:

    $$
    $$$$D\_i = \\pi\_0 + \\pi\_1 Z\_i + \\pi\_2 X\_i + \\nu\_i

    $$
    $$$$
    $$
2.  **Second Stage**: Regress recovery time $Y\_i$ on the predicted hospital quality $\\hat{D}\_i$ and covariates:

    $$
    $$$$Y\_i = \\alpha\_0 + \\alpha\_1 \\hat{D}\_i + \\alpha\_2 X\_i + \\varepsilon\_i

    $$
    $$$$
    $$The coefficient $\\alpha\_1$ estimates the LATE, adjusted for covariates like age or comorbidities to improve precision.

### Interpretation

Suppose the IV estimate suggests that high-quality hospitals reduce recovery time by 5 days for compliers. This effect applies to patients whose hospital choice is swayed by proximity, not necessarily all patients. For instance, patients who always seek high-quality hospitals (regardless of distance) or those who never do are excluded from this estimate. This specificity is a key feature of IV, distinguishing it from methods targeting broader effects.

### Practical Considerations

The validity of the IV approach hinges on the instrument’s assumptions. Relevance can be checked by ensuring a strong first-stage relationship (e.g., an F-statistic \> 10 for $\\pi\_1$). The exclusion restriction is harder to verify; we must argue that proximity doesn’t affect recovery through other channels, like local healthcare infrastructure. Independence requires that proximity isn’t correlated with unmeasured factors like wealth, which might be plausible after controlling for observables like income or education. Sensitivity analyses or falsification tests (e.g., checking if proximity affects outcomes in a non-surgical context) can bolster confidence. Weak instruments, where proximity only marginally affects hospital choice, could lead to imprecise estimates, necessitating robust methods like limited-information maximum likelihood.

### Comparison with Other Causal Inference Methods

Referring to the updated **Summary Table: Techniques for Causal Statistical Analysis**, IV differs from Double Machine Learning (DML) and Difference-in-Differences (DiD) in key ways. DML assumes conditional ignorability (${Y\_i(1), Y\_i(0)} \\perp D\_i \\mid X\_i$) and uses machine learning to model nuisance functions (outcome and propensity score) in high-dimensional settings. In the hospital quality example, DML would require all confounders (e.g., health consciousness) to be observed and included in $X\_i$, estimating the ATE for all patients. IV, by contrast, handles unmeasured confounding via the instrument (proximity) but estimates the LATE for compliers, requiring no assumption about observed confounders but a valid instrument. DML is more robust to model misspecification but fails if key confounders are unmeasured, unlike IV.

DiD, which relies on parallel trends ($\\mathbb{E}[Y\_{i1}(0) - Y\_{i0}(0) \\mid D\_i = 1] = \\mathbb{E}[Y\_{i1}(0) - Y\_{i0}(0) \\mid D\_i = 0]$), is suited for settings with pre- and post-treatment data. If we were studying a policy improving hospital quality in certain regions, DiD could compare recovery time changes between treated and control regions. However, DiD assumes no differential trends in unmeasured confounders, which may not hold if high-quality hospitals attract healthier patients over time. IV avoids this by using proximity as an exogenous shock, but it requires a valid instrument, which DiD does not.

## Simulation and R Implementation

We now simulate a valid instrument that affects treatment but not the outcome directly:

```r
set.seed(456)
n <- 2000
z <- rbinom(n, 1, 0.5) # Instrument
x <- rnorm(n)
d <- rbinom(n, 1, plogis(0.4 * z + 0.5 * x)) # Treatment influenced by Z
y <- 1 + 2 * d + 1.5 * x + rnorm(n) # Outcome depends on D and X

iv_data <- data.frame(y, d, z, x)

# First stage: check relevance
summary(lm(d ~ z + x, data = iv_data))

# 2SLS using AER package
library(AER)
iv_mod <- ivreg(y ~ d + x | z + x, data = iv_data)
summary(iv_mod)
```

The coefficient on `d` is the IV estimate of the treatment effect. Note that `z + x` in the second part of the formula specifies instruments (`z`) and exogenous controls (`x`).

### R Code for Healthcare Example

```r
# Install and load required packages
if (!requireNamespace("AER", quietly = TRUE)) install.packages("AER")
if (!requireNamespace("ggplot2", quietly = TRUE)) install.packages("ggplot2")
library(AER)
library(ggplot2)

# Set seed for reproducibility
set.seed(123)

# Simulate synthetic data
n <- 1000 # Number of observations
p <- 5    # Number of covariates
X <- matrix(rnorm(n * p), nrow = n) # Covariates (e.g., age, comorbidities)
colnames(X) <- paste0("X", 1:p)

# Instrument: Z = 1 if within 10 miles of high-quality hospital
Z <- rbinom(n, 1, 0.5) # Binary instrument, ~50% live near high-quality hospital

# Treatment: D = 1 if treated at high-quality hospital
# First stage: D depends on Z and covariates
pi_1 <- 0.8 # Strong instrument effect
D <- as.numeric(runif(n) < plogis(0.5 + pi_1 * Z + 0.2 * rowSums(X[, 1:2])))

# Outcome: Recovery time (days, lower is better)
true_late <- -5 # True LATE: high-quality hospital reduces recovery by 5 days
Y <- 20 + true_late * D + 2 * rowSums(X[, 1:3]) + rnorm(n, sd = 2)

# Create data frame
data <- data.frame(Y = Y, D = D, Z = Z, X)

# First stage: Check instrument relevance
first_stage <- lm(D ~ Z + ., data = data[, c("D", "Z", paste0("X", 1:p))])
cat("First-stage F-statistic:", summary(first_stage)$fstatistic[1], "\n")

# IV estimation using 2SLS
iv_model <- ivreg(Y ~ D + . | Z + ., data = data[, c("Y", "D", "Z", paste0("X", 1:p))])
iv_results <- summary(iv_model)
cat("2SLS LATE estimate:", coef(iv_results)["D", "Estimate"], "\n")
cat("Standard error:", coef(iv_results)["D", "Std. Error"], "\n")

# Wald estimator for comparison
y_z1 <- mean(data$Y[data$Z == 1]) # Mean outcome when Z = 1
y_z0 <- mean(data$Y[data$Z == 0]) # Mean outcome when Z = 0
d_z1 <- mean(data$D[data$Z == 1]) # Mean treatment when Z = 1
d_z0 <- mean(data$D[data$Z == 0]) # Mean treatment when Z = 0
wald_estimate <- (y_z1 - y_z0) / (d_z1 - d_z0)
cat("Wald estimator LATE:", wald_estimate, "\n")

# Naive OLS for comparison
ols_model <- lm(Y ~ D + ., data = data[, c("Y", "D", paste0("X", 1:p))])
ols_estimate <- coef(ols_model)["D"]
cat("Naive OLS estimate:", ols_estimate, "\n")

# Visualize results
estimates <- data.frame(
  Method = c("2SLS", "Wald", "OLS"),
  Estimate = c(coef(iv_results)["D", "Estimate"], wald_estimate, ols_estimate),
  SE = c(coef(iv_results)["D", "Std. Error"], NA, summary(ols_model)$coefficients["D", "Std. Error"]),
  Lower = c(coef(iv_results)["D", "Estimate"] - 1.96 * coef(iv_results)["D", "Std. Error"],
            NA, ols_estimate - 1.96 * summary(ols_model)$coefficients["D", "Std. Error"]),
  Upper = c(coef(iv_results)["D", "Estimate"] + 1.96 * coef(iv_results)["D", "Std. Error"],
            NA, ols_estimate + 1.96 * summary(ols_model)$coefficients["D", "Std. Error"])
)

ggplot(estimates, aes(x = Method, y = Estimate, fill = Method)) +
  geom_bar(stat = "identity", alpha = 0.6) +
  geom_errorbar(aes(ymin = Lower, ymax = Upper), width = 0.2, na.rm = TRUE) +
  geom_hline(yintercept = true_late, linetype = "dashed", color = "red") +
  labs(title = "LATE Estimates: Hospital Quality on Recovery Time",
       y = "Estimated Effect (Days)", x = "Method") +
  scale_fill_manual(values = c("2SLS" = "#1f77b4", "Wald" = "#ff7f0e", "OLS" = "#2ca02c")) +
  theme_minimal() +
  annotate("text", x = 3.5, y = true_late + 0.5, label = "True LATE", color = "red")
```

### Explanation of Code

The code simulates a dataset with $n = 1000$ patients, a binary instrument $Z\_i$ (proximity to a high-quality hospital), a binary treatment $D\_i$ (high-quality vs. standard hospital), and a continuous outcome $Y\_i$ (recovery time in days). Five covariates $X\_i$ (e.g., age, comorbidities) are included to mimic realistic confounding. The true LATE is set to $-5$, meaning high-quality hospitals reduce recovery time by 5 days for compliers. The instrument is designed to be strong ($\\pi\_1 = 0.8$), satisfying the relevance assumption ($\\text{Cov}(Z\_i, D\_i) \\neq 0$).

The first stage regresses $D\_i$ on $Z\_i$ and covariates to verify relevance, reporting the F-statistic. The IV effect is estimated using 2SLS via the `ivreg` function from the `AER` package, implementing:

$$D_i = \pi_0 + \pi_1 Z_i + \pi_2 X_i + \nu_i$$
$$Y_i = \alpha_0 + \alpha_1 \hat{D}_i + \alpha_2 X_i + \varepsilon_i$$

The Wald estimator is computed manually as:

$$\widehat{\text{LATE}} = \frac{\mathbb{E}[Y_i \mid Z_i = 1] - \mathbb{E}[Y_i \mid Z_i = 0]}{\mathbb{E}[D_i \mid Z_i = 1] - \mathbb{E}[D_i \mid Z_i = 0]}$$

A naive OLS regression is included for comparison, likely biased due to unmeasured confounding (simulated via noise and covariate effects). The results are visualized in a bar chart comparing 2SLS, Wald, and OLS estimates, with error bars for 2SLS and OLS (Wald standard errors are omitted for simplicity).

### Expected Output

  - **First Stage**: The F-statistic should exceed 10, confirming relevance.
  - **2SLS**: The LATE estimate ($\\alpha\_1$) should be close to $-5$, with a standard error and confidence interval.
  - **Wald Estimator**: Should approximate the 2SLS estimate, though less precise without covariates.
  - **OLS**: Likely biased (e.g., attenuated toward zero) due to confounding.
  - **Plot**: A bar chart showing the three estimates, with the true LATE ($-5$) as a red dashed line. 2SLS and Wald estimates should be near $-5$, while OLS may deviate.

### Diagnostics

Key diagnostics include:

  * **First-stage F-statistic** (should exceed 10): tests instrument strength.
  * **Overidentification tests**: e.g., Hansen J-test, for models with multiple instruments.
  * **Placebo or falsification checks**: assess exclusion by testing instrument association with outcomes that should not be affected.

### Summary Table

| Method                    | Key Assumption                                                 | Target Estimand                  | R Implementation        | Common Use Cases                                      |
| :------------------------ | :------------------------------------------------------------- | :------------------------------- | :---------------------- | :---------------------------------------------------- |
| Difference-in-Differences | Parallel trends (untreated units represent counterfactual trend) | ATE (under strong assumptions)   | `lm(y ~ treat*time)`    | Policy evaluation, natural experiments                |
| Instrumental Variables    | Exclusion restriction, relevance, independence                 | LATE (compliers only)            | `ivreg()` from **AER** | Endogenous treatment, natural experiments             |

### Notes

  - **Assumptions**: The simulation assumes exclusion ($Z\_i$ affects $Y\_i$ only through $D\_i$) and independence ($Z\_i \\perp {Y\_i(1), Y\_i(0)}$) by design. In practice, you’d need to justify these (e.g., proximity doesn’t affect recovery via other channels like local healthcare quality).
  - **Extensions**: To explore heterogeneous effects, you could interact $D\_i$ with covariates in the second stage. For robustness, add tests for weak instruments or sensitivity analyses.
  - **Packages**: The `AER` package is used for 2SLS; alternatives like `ivpack` offer additional diagnostics.

## Conclusion

Instrumental Variables is a sophisticated and powerful quasi-experimental method for causal inference, particularly indispensable when unobserved confounding biases ordinary least squares estimates. By carefully selecting and validating an instrumental variable that satisfies the relevance, exogeneity, and exclusion restriction assumptions, researchers can robustly estimate causal effects. Our healthcare example demonstrated how IV could be applied to estimate the impact of a medical procedure on patient recovery, providing practical R code for implementation. While challenging to implement due to the strict requirements for a valid instrument, IV offers a crucial approach to isolating causal effects in complex observational settings. In future posts, we will continue exploring advanced causal inference techniques, complementing methods like DiD and IV to build a comprehensive toolkit for empirical research.

## References

  - Angrist, J. D., & Pischke, J.-S. (2009). *Mostly Harmless Econometrics: An Empiricist's Companion*. Princeton University Press.
  - Wooldridge, J. M. (2010). *Econometric Analysis of Cross Section and Panel Data*. MIT Press.
  - Imbens, G. W., & Rubin, D. B. (2015). *Causal Inference for Statistics, Social, and Biomedical Sciences: An Introduction*. Cambridge University Press.
  - Hernán, M. A., & Robins, J. M. (2020). *Causal Inference: What If*. Chapman & Hall/CRC.
