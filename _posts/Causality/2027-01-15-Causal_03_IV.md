
### **Instrumental Variables (IV)**

### Instrumental Variables (IV) Estimation

#### Motivation and Identification

Instrumental Variables (IV) methods offer a strategy for causal inference in the presence of **unmeasured confounding**, a situation where the standard assumption of conditional ignorability fails. That is, when potential outcomes $Y(1), Y(0)$ are not conditionally independent of treatment $D$ given observed covariates $X$, alternative identification strategies are required. IV methods achieve this by exploiting exogenous variation in treatment induced by an external variable, known as an **instrument** $Z$, which must satisfy a set of well-defined assumptions.

The validity of an instrumental variable relies on the following three core conditions:

1. **Relevance**: The instrument must have a non-zero correlation with the treatment variable. Formally,

   $$
   \operatorname{Cov}(Z, D) \neq 0,
   $$

   which ensures that the instrument induces variation in the treatment.

2. **Exclusion Restriction**: The instrument must affect the outcome $Y$ only through its effect on the treatment $D$. That is, conditional on the treatment and covariates, the instrument has no direct effect on the outcome:

   $$
   Y = f(D, X, \varepsilon), \quad \text{with } Z \notin f.
   $$

3. **Independence (or Exogeneity)**: The instrument must be independent of the unobserved determinants of the outcome, i.e., the instrument is as good as randomly assigned:

   $$
   Z \perp \{Y(1), Y(0)\} \mid X.
   $$

When these conditions are met, the IV approach identifies the **Local Average Treatment Effect (LATE)**—the average treatment effect for the subpopulation of **compliers**, individuals whose treatment status is influenced by the instrument. It is important to emphasize that LATE may differ from the Average Treatment Effect (ATE), as it pertains only to a specific subpopulation determined by the instrument’s influence.

#### Theoretical Estimation Framework

In the case of binary instruments and binary treatments, the IV estimand can be expressed via the **Wald estimator**, given by:

$$
\widehat{\text{LATE}} = \frac{\mathbb{E}[Y \mid Z = 1] - \mathbb{E}[Y \mid Z = 0]}{\mathbb{E}[D \mid Z = 1] - \mathbb{E}[D \mid Z = 0]}.
$$

This ratio captures the causal effect among compliers by scaling the change in outcomes induced by the instrument by the change in treatment uptake.

In more general settings, particularly with continuous instruments or treatments and with covariates $X$, the causal effect is commonly estimated via **two-stage least squares (2SLS)**. The procedure consists of two sequential regressions:

* **First stage**: Estimate the predicted treatment value $\hat{D}_i$ using the instrument:

  $$
  D_i = \pi_0 + \pi_1 Z_i + \pi_2^\top X_i + \nu_i.
  $$

* **Second stage**: Regress the outcome $Y_i$ on the predicted treatment:

  $$
  Y_i = \alpha_0 + \alpha_1 \hat{D}_i + \alpha_2^\top X_i + \varepsilon_i.
  $$

The coefficient $\alpha_1$ from the second stage provides a consistent estimate of the causal effect under the IV assumptions. Importantly, this estimator remains consistent even in the presence of omitted variable bias, provided the instrument is valid.

Instrumental Variables (IV) estimation differs substantially from other causal inference strategies, such as **propensity score methods** and **Difference-in-Differences (DiD)**, both in terms of identifying assumptions and target estimands.

Propensity score matching and related methods (e.g., inverse probability weighting, doubly robust estimators) rely on the **conditional ignorability assumption**, i.e., $Y(1), Y(0) \perp D \mid X$, which requires that all confounders influencing both treatment and outcome are observed and properly adjusted for. These approaches target the **Average Treatment Effect (ATE)** or the **Average Treatment effect on the Treated (ATT)** across the full population or a matched subpopulation. In contrast, IV methods do not require all confounders to be measured but instead depend on the presence of a valid instrument and identify LATE, a more restricted estimand.

Difference-in-Differences (DiD) methods exploit temporal variation in treatment exposure across groups and require the **parallel trends assumption**: that, in the absence of treatment, the difference in outcomes between treated and control groups would have remained constant over time. DiD can accommodate some unobserved confounding, provided it is time-invariant. However, DiD assumes the absence of differential trends, an assumption that cannot be tested directly. Unlike IV, DiD does not rely on an instrument but on quasi-experimental variation in timing or assignment. IV estimation offers a powerful alternative when unmeasured confounding invalidates conditional ignorability, but it comes with its own limitations, such as the difficulty of finding valid instruments and the interpretation of effects limited to compliers. While DiD and propensity score methods may estimate broader effects under stronger assumptions, IV provides robustness to hidden bias at the cost of narrower causal generalizability.


#### **Simulation and R Implementation**

We now simulate a valid instrument that affects treatment but not the outcome directly:

```r
set.seed(456)
n <- 2000
z <- rbinom(n, 1, 0.5)  # Instrument
x <- rnorm(n)
d <- rbinom(n, 1, plogis(0.4 * z + 0.5 * x))  # Treatment influenced by Z
y <- 1 + 2 * d + 1.5 * x + rnorm(n)  # Outcome depends on D and X

iv_data <- data.frame(y, d, z, x)

# First stage: check relevance
summary(lm(d ~ z + x, data = iv_data))

# 2SLS using AER package
library(AER)
iv_mod <- ivreg(y ~ d + x | z + x, data = iv_data)
summary(iv_mod)
```

The coefficient on `d` is the IV estimate of the treatment effect. Note that `z + x` in the second part of the formula specifies instruments (`z`) and exogenous controls (`x`).

#### **Diagnostics**

Key diagnostics include:

* **First-stage F-statistic** (should exceed 10): tests instrument strength.
* **Overidentification tests**: e.g., Hansen J-test, for models with multiple instruments.
* **Placebo or falsification checks**: assess exclusion by testing instrument association with outcomes that should not be affected.


### **Summary Table**

| Method                    | Key Assumption                                                   | Target Estimand                | R Implementation       | Common Use Cases                          |
| ------------------------- | ---------------------------------------------------------------- | ------------------------------ | ---------------------- | ----------------------------------------- |
| Difference-in-Differences | Parallel trends (untreated units represent counterfactual trend) | ATE (under strong assumptions) | `lm(y ~ treat*time)`   | Policy evaluation, natural experiments    |
| Instrumental Variables    | Exclusion restriction, relevance, independence                   | LATE (compliers only)          | `ivreg()` from **AER** | Endogenous treatment, natural experiments |

### Healthcare Example: Estimating the Causal Effect of Hospital Quality on Patient Recovery

#### Context and Problem
Suppose we want to estimate the causal effect of hospital quality (treatment) on patient recovery time after surgery (outcome). For simplicity, let’s define hospital quality as a binary variable \( D_i \in \{0, 1\} \), where \( D_i = 1 \) indicates a patient is treated at a high-quality hospital (e.g., one with advanced facilities and experienced staff) and \( D_i = 0 \) indicates a standard hospital. The outcome \( Y_i \) is the number of days until recovery, and lower values indicate faster recovery. A naive approach might compare recovery times between patients at high-quality versus standard hospitals using regression:

\[
Y_i = \beta_0 + \beta_1 D_i + \beta_2 X_i + \epsilon_i
\]

where \( X_i \) includes observed confounders like age, sex, and pre-existing conditions. However, unmeasured confounding—such as patients’ socioeconomic status or health consciousness—may bias \( \beta_1 \). For example, wealthier or more health-conscious patients might choose high-quality hospitals *and* recover faster due to better overall health, not hospital quality.

#### Instrumental Variables Approach
To address this, we use IV estimation, leveraging an instrument \( Z_i \) that influences hospital choice but not recovery time except through hospital quality. A plausible instrument is **geographic proximity to a high-quality hospital**, defined as a binary variable \( Z_i = 1 \) if the patient lives within 10 miles of a high-quality hospital and \( Z_i = 0 \) otherwise. The intuition is that patients closer to a high-quality hospital are more likely to choose it due to convenience, but proximity itself does not directly affect recovery time (except through hospital choice).

The IV framework requires three assumptions, as outlined in your prior query:

1. **Relevance**: Proximity must influence hospital choice:

\[
\text{Cov}(Z_i, D_i) \neq 0
\]

Patients living closer to a high-quality hospital are more likely to attend it, perhaps due to lower travel costs or familiarity. This can be tested empirically by checking if \( Z_i \) predicts \( D_i \) in a first-stage regression.

2. **Exclusion Restriction**: Proximity affects recovery time \( Y_i \) only through hospital choice \( D_i \), not directly or via other channels. For instance, living near a high-quality hospital shouldn’t influence recovery through factors like local healthcare access, assuming similar baseline care quality across regions.

3. **Independence**: Proximity is “as good as randomly assigned,” independent of potential outcomes \( Y_i(1), Y_i(0) \):

\[
Z_i \perp \{Y_i(1), Y_i(0)\}
\]

This assumes proximity is not correlated with unmeasured confounders like patient health consciousness, which holds if residential patterns are unrelated to health behaviors after controlling for observed covariates \( X_i \).

Under these assumptions, IV estimates the **Local Average Treatment Effect (LATE)**, the causal effect of high-quality hospital care on recovery time for **compliers**—patients who choose a high-quality hospital because they live nearby but would choose a standard hospital if they lived farther away.

#### Formal Estimation
The IV effect can be estimated using the **Wald estimator** for binary \( Z_i \) and \( D_i \):

\[
\widehat{\text{LATE}} = \frac{\mathbb{E}[Y_i \mid Z_i = 1] - \mathbb{E}[Y_i \mid Z_i = 0]}{\mathbb{E}[D_i \mid Z_i = 1] - \mathbb{E}[D_i \mid Z_i = 0]}
\]

The numerator is the difference in average recovery times between patients living near versus far from a high-quality hospital (the reduced-form effect). The denominator is the difference in the probability of attending a high-quality hospital based on proximity (the first-stage effect). For example, if patients near a high-quality hospital recover 2 days faster on average, and proximity increases the likelihood of attending a high-quality hospital by 0.4, the LATE is \(-2 / 0.4 = -5\) days, meaning high-quality hospitals reduce recovery time by 5 days for compliers.

Alternatively, we can use **two-stage least squares (2SLS)** for more flexibility, especially with continuous outcomes or covariates:

1. **First Stage**: Regress hospital quality \( D_i \) on proximity \( Z_i \) and covariates \( X_i \):

\[
D_i = \pi_0 + \pi_1 Z_i + \pi_2 X_i + \nu_i
\]

2. **Second Stage**: Regress recovery time \( Y_i \) on the predicted hospital quality \( \hat{D}_i \) and covariates:

\[
Y_i = \alpha_0 + \alpha_1 \hat{D}_i + \alpha_2 X_i + \varepsilon_i
\]

The coefficient \( \alpha_1 \) estimates the LATE, adjusted for covariates like age or comorbidities to improve precision.

#### Interpretation
Suppose the IV estimate suggests that high-quality hospitals reduce recovery time by 5 days for compliers. This effect applies to patients whose hospital choice is swayed by proximity, not necessarily all patients. For instance, patients who always seek high-quality hospitals (regardless of distance) or those who never do are excluded from this estimate. This specificity is a key feature of IV, distinguishing it from methods targeting broader effects.

#### Practical Considerations
The validity of the IV approach hinges on the instrument’s assumptions. Relevance can be checked by ensuring a strong first-stage relationship (e.g., an F-statistic > 10 for \( \pi_1 \)). The exclusion restriction is harder to verify; we must argue that proximity doesn’t affect recovery through other channels, like local healthcare infrastructure. Independence requires that proximity isn’t correlated with unmeasured factors like wealth, which might be plausible after controlling for observables like income or education. Sensitivity analyses or falsification tests (e.g., checking if proximity affects outcomes in a non-surgical context) can bolster confidence. Weak instruments, where proximity only marginally affects hospital choice, could lead to imprecise estimates, necessitating robust methods like limited-information maximum likelihood.

#### Comparison with DML and DiD
Referring to the updated **Summary Table: Techniques for Causal Statistical Analysis**, IV differs from Double Machine Learning (DML) and Difference-in-Differences (DiD) in key ways. DML, as described in your prior query, assumes conditional ignorability (\( \{Y_i(1), Y_i(0)\} \perp D_i \mid X_i \)) and uses machine learning to model nuisance functions (outcome and propensity score) in high-dimensional settings. In the hospital quality example, DML would require all confounders (e.g., health consciousness) to be observed and included in \( X_i \), estimating the ATE for all patients. IV, by contrast, handles unmeasured confounding via the instrument (proximity) but estimates the LATE for compliers, requiring no assumption about observed confounders but a valid instrument. DML is more robust to model misspecification but fails if key confounders are unmeasured, unlike IV.

DiD, which relies on parallel trends (\( \mathbb{E}[Y_{i1}(0) - Y_{i0}(0) \mid D_i = 1] = \mathbb{E}[Y_{i1}(0) - Y_{i0}(0) \mid D_i = 0] \)), is suited for settings with pre- and post-treatment data. If we were studying a policy improving hospital quality in certain regions, DiD could compare recovery time changes between treated and control regions. However, DiD assumes no differential trends in unmeasured confounders, which may not hold if high-quality hospitals attract healthier patients over time. IV avoids this by using proximity as an exogenous shock, but it requires a valid instrument, which DiD does not.

#### Conclusion
This IV example—using proximity to estimate the effect of hospital quality on recovery time—illustrates how IV addresses unmeasured confounding in healthcare. By leveraging an external source of variation, IV provides a causal estimate for compliers, offering a robust alternative when randomization is infeasible. Compared to DML, IV relaxes the need for all confounders to be observed but demands a valid instrument. Compared to DiD, IV doesn’t rely on temporal data or parallel trends, making it suitable for cross-sectional studies. The approach’s strength lies in its ability to handle endogeneity, but its reliance on strong assumptions about the instrument underscores the need for careful justification and robustness checks. If you’d like a deeper exploration (e.g., testing assumptions or extending to heterogeneous effects), let me know!


Below, I provide R code to implement the Instrumental Variables (IV) approach for the healthcare example described in your previous query: estimating the causal effect of hospital quality (\( D_i \), binary: high-quality vs. standard hospital) on patient recovery time (\( Y_i \), continuous: days until recovery) using geographic proximity to a high-quality hospital (\( Z_i \), binary: living within 10 miles vs. not) as the instrument. The code simulates synthetic data, estimates the IV effect using two-stage least squares (2SLS) and the Wald estimator, and visualizes the results. The implementation aligns with the mathematical formalism from your IV query and the context from the healthcare example, ensuring clarity and reproducibility. Comments explain each step, and the code is concise yet comprehensive, incorporating covariates to mirror the example’s realism.

### R Code

```R
# Install and load required packages
if (!requireNamespace("AER", quietly = TRUE)) install.packages("AER")
if (!requireNamespace("ggplot2", quietly = TRUE)) install.packages("ggplot2")
library(AER)
library(ggplot2)

# Set seed for reproducibility
set.seed(123)

# Simulate synthetic data
n <- 1000  # Number of observations
p <- 5     # Number of covariates
X <- matrix(rnorm(n * p), nrow = n)  # Covariates (e.g., age, comorbidities)
colnames(X) <- paste0("X", 1:p)

# Instrument: Z = 1 if within 10 miles of high-quality hospital
Z <- rbinom(n, 1, 0.5)  # Binary instrument, ~50% live near high-quality hospital

# Treatment: D = 1 if treated at high-quality hospital
# First stage: D depends on Z and covariates
pi_1 <- 0.8  # Strong instrument effect
D <- as.numeric(runif(n) < plogis(0.5 + pi_1 * Z + 0.2 * rowSums(X[, 1:2])))

# Outcome: Recovery time (days, lower is better)
true_late <- -5  # True LATE: high-quality hospital reduces recovery by 5 days
Y <- 20 + true_late * D + 2 * rowSums(X[, 1:3]) + rnorm(n, sd = 2)

# Create data frame
data <- data.frame(Y = Y, D = D, Z = Z, X)

# First stage: Check instrument relevance
first_stage <- lm(D ~ Z + ., data = data[, c("D", "Z", paste0("X", 1:p))])
cat("First-stage F-statistic:", summary(first_stage)$fstatistic[1], "\n")

# IV estimation using 2SLS
iv_model <- ivreg(Y ~ D + . | Z + ., data = data[, c("Y", "D", "Z", paste0("X", 1:p))])
iv_results <- summary(iv_model)
cat("2SLS LATE estimate:", coef(iv_results)["D", "Estimate"], "\n")
cat("Standard error:", coef(iv_results)["D", "Std. Error"], "\n")

# Wald estimator for comparison
y_z1 <- mean(data$Y[data$Z == 1])  # Mean outcome when Z = 1
y_z0 <- mean(data$Y[data$Z == 0])  # Mean outcome when Z = 0
d_z1 <- mean(data$D[data$Z == 1])  # Mean treatment when Z = 1
d_z0 <- mean(data$D[data$Z == 0])  # Mean treatment when Z = 0
wald_estimate <- (y_z1 - y_z0) / (d_z1 - d_z0)
cat("Wald estimator LATE:", wald_estimate, "\n")

# Naive OLS for comparison
ols_model <- lm(Y ~ D + ., data = data[, c("Y", "D", paste0("X", 1:p))])
ols_estimate <- coef(ols_model)["D"]
cat("Naive OLS estimate:", ols_estimate, "\n")

# Visualize results
estimates <- data.frame(
  Method = c("2SLS", "Wald", "OLS"),
  Estimate = c(coef(iv_results)["D", "Estimate"], wald_estimate, ols_estimate),
  SE = c(coef(iv_results)["D", "Std. Error"], NA, summary(ols_model)$coefficients["D", "Std. Error"]),
  Lower = c(coef(iv_results)["D", "Estimate"] - 1.96 * coef(iv_results)["D", "Std. Error"], 
            NA, ols_estimate - 1.96 * summary(ols_model)$coefficients["D", "Std. Error"]),
  Upper = c(coef(iv_results)["D", "Estimate"] + 1.96 * coef(iv_results)["D", "Std. Error"], 
            NA, ols_estimate + 1.96 * summary(ols_model)$coefficients["D", "Std. Error"])
)

ggplot(estimates, aes(x = Method, y = Estimate, fill = Method)) +
  geom_bar(stat = "identity", alpha = 0.6) +
  geom_errorbar(aes(ymin = Lower, ymax = Upper), width = 0.2, na.rm = TRUE) +
  geom_hline(yintercept = true_late, linetype = "dashed", color = "red") +
  labs(title = "LATE Estimates: Hospital Quality on Recovery Time", 
       y = "Estimated Effect (Days)", x = "Method") +
  scale_fill_manual(values = c("2SLS" = "#1f77b4", "Wald" = "#ff7f0e", "OLS" = "#2ca02c")) +
  theme_minimal() +
  annotate("text", x = 3.5, y = true_late + 0.5, label = "True LATE", color = "red")
```

### Explanation of Code

The code simulates a dataset with \( n = 1000 \) patients, a binary instrument \( Z_i \) (proximity to a high-quality hospital), a binary treatment \( D_i \) (high-quality vs. standard hospital), and a continuous outcome \( Y_i \) (recovery time in days). Five covariates \( X_i \) (e.g., age, comorbidities) are included to mimic realistic confounding. The true LATE is set to \(-5\), meaning high-quality hospitals reduce recovery time by 5 days for compliers. The instrument is designed to be strong (\( \pi_1 = 0.8 \)), satisfying the relevance assumption (\( \text{Cov}(Z_i, D_i) \neq 0 \)).

The first stage regresses \( D_i \) on \( Z_i \) and covariates to verify relevance, reporting the F-statistic. The IV effect is estimated using 2SLS via the `ivreg` function from the `AER` package, implementing:

\[
D_i = \pi_0 + \pi_1 Z_i + \pi_2 X_i + \nu_i
\]
\[
Y_i = \alpha_0 + \alpha_1 \hat{D}_i + \alpha_2 X_i + \varepsilon_i
\]

The Wald estimator is computed manually as:

\[
\widehat{\text{LATE}} = \frac{\mathbb{E}[Y_i \mid Z_i = 1] - \mathbb{E}[Y_i \mid Z_i = 0]}{\mathbb{E}[D_i \mid Z_i = 1] - \mathbb{E}[D_i \mid Z_i = 0]}
\]

A naive OLS regression is included for comparison, likely biased due to unmeasured confounding (simulated via noise and covariate effects). The results are visualized in a bar chart comparing 2SLS, Wald, and OLS estimates, with error bars for 2SLS and OLS (Wald standard errors are omitted for simplicity).

### Expected Output

- **First Stage**: The F-statistic should exceed 10, confirming relevance.
- **2SLS**: The LATE estimate (\( \alpha_1 \)) should be close to \(-5\), with a standard error and confidence interval.
- **Wald Estimator**: Should approximate the 2SLS estimate, though less precise without covariates.
- **OLS**: Likely biased (e.g., attenuated toward zero) due to confounding.
- **Plot**: A bar chart showing the three estimates, with the true LATE (\(-5\)) as a red dashed line. 2SLS and Wald estimates should be near \(-5\), while OLS may deviate.

### Notes

- **Assumptions**: The simulation assumes exclusion (\( Z_i \) affects \( Y_i \) only through \( D_i \)) and independence (\( Z_i \perp \{Y_i(1), Y_i(0)\} \)) by design. In practice, you’d need to justify these (e.g., proximity doesn’t affect recovery via other channels like local healthcare quality).
- **Extensions**: To explore heterogeneous effects, you could interact \( D_i \) with covariates in the second stage. For robustness, add tests for weak instruments or sensitivity analyses.
- **Packages**: The `AER` package is used for 2SLS; alternatives like `ivpack` offer additional diagnostics.
